"""
W&B ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„± ëª¨ë“ˆ

í‰ê°€ ê²°ê³¼ë¥¼ W&B Tableë¡œ ë³€í™˜í•˜ì—¬ ë¡œê¹…í•©ë‹ˆë‹¤.
README_ko.mdì˜ íƒì†Œë…¸ë¯¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GLP/ALT ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
"""

from typing import Any

import pandas as pd
import wandb


# ë²¤ì¹˜ë§ˆí¬ â†’ íƒì†Œë…¸ë¯¸ ë§¤í•‘
BENCHMARK_TAXONOMY = {
    # GLP - êµ¬ë¬¸í•´ì„
    "ko_balt_700_syntax": {
        "category": "GLP_êµ¬ë¬¸í•´ì„",
        "score_key": "score",
    },
    # GLP - ì˜ë¯¸í•´ì„
    "ko_balt_700_semantic": {
        "category": "GLP_ì˜ë¯¸í•´ì„",
        "score_key": "score",
    },
    "haerae_bench_v1_rc": {
        "category": "GLP_ì˜ë¯¸í•´ì„",
        "score_key": "score",
    },
    # GLP - í‘œí˜„
    "ko_mtbench": {
        "category": "GLP_í‘œí˜„",
        "score_key": "score",
        "sub_scores": {
            # ì‹¤ì œ ë©”íŠ¸ë¦­ ì´ë¦„: mtbench_scorer_XXX_score
            "mtbench_scorer_roleplay_score": "GLP_í‘œí˜„",
            "mtbench_scorer_humanities_score": "GLP_í‘œí˜„",
            "mtbench_scorer_writing_score": "GLP_í‘œí˜„",
            "mtbench_scorer_reasoning_score": "GLP_ë…¼ë¦¬ì ì¶”ë¡ ",
            # "mtbench_scorer_coding_score": "GLP_ì½”ë”©ëŠ¥ë ¥",
            # "mtbench_scorer_math_score": "GLP_ìˆ˜í•™ì ì¶”ë¡ ",
            # "mtbench_scorer_stem_score": "GLP_ì „ë¬¸ì ì§€ì‹",
            # "mtbench_scorer_extraction_score": "GLP_ì •ë³´ê²€ìƒ‰",
        },
    },
    # GLP - ì •ë³´ê²€ìƒ‰
    "squad_kor_v1": {
        "category": "GLP_ì •ë³´ê²€ìƒ‰",
        "score_key": "score",
    },
    # GLP - ì¼ë°˜ì ì§€ì‹
    "kmmlu": {
        "category": "GLP_ì¼ë°˜ì ì§€ì‹",
        "score_key": "score",
    },
    "haerae_bench_v1_wo_rc": {
        "category": "GLP_ì¼ë°˜ì ì§€ì‹",
        "score_key": "score",
    },
    # GLP - ì „ë¬¸ì ì§€ì‹
    "kmmlu_pro": {
        "category": "GLP_ì „ë¬¸ì ì§€ì‹",
        "score_key": "score",
    },
    "ko_hle": {
        "category": "GLP_ì „ë¬¸ì ì§€ì‹",
        "score_key": "score",
    },
    # GLP - ë…¼ë¦¬ì ì¶”ë¡  (ìƒì‹ì¶”ë¡  í¬í•¨)
    "ko_hellaswag": {
        "category": "GLP_ë…¼ë¦¬ì ì¶”ë¡ ",
        "score_key": "score",
    },
    # GLP - ìˆ˜í•™ì ì¶”ë¡ 
    "hrm8k": {
        "category": "GLP_ìˆ˜í•™ì ì¶”ë¡ ",
        "score_key": "score",
    },
    "ko_aime2025": {
        "category": "GLP_ìˆ˜í•™ì ì¶”ë¡ ",
        "score_key": "score",
    },
    # GLP - ì¶”ìƒì ì¶”ë¡ 
    "ko_arc_agi": {
        "category": "GLP_ì¶”ìƒì ì¶”ë¡ ",
        "score_key": "score",
    },
    # GLP - ì½”ë”©ëŠ¥ë ¥
    "swebench_verified_official_80": {
        "category": "GLP_ì½”ë”©ëŠ¥ë ¥",
        "score_key": "score",
    },
    "humaneval_100": {
        "category": "GLP_ì½”ë”©ëŠ¥ë ¥",
        "score_key": "score",
    },
    "bigcodebench_100": {
        "category": "GLP_ì½”ë”©ëŠ¥ë ¥",
        "score_key": "score",
    },
    # GLP - í•¨ìˆ˜í˜¸ì¶œ
    "bfcl": {
        "category": "GLP_í•¨ìˆ˜í˜¸ì¶œ",
        "score_key": "score",
    },
    # ALT - ì œì–´ì„±
    "ifeval_ko": {
        "category": "ALT_ì œì–´ì„±",
        "score_key": "score",
    },
    # ALT - ìœ¤ë¦¬/ë„ë•
    "ko_moral": {
        "category": "ALT_ìœ¤ë¦¬/ë„ë•",
        "score_key": "score",
    },
    # ALT - ìœ í•´ì„±ë°©ì§€
    "korean_hate_speech": {
        "category": "ALT_ìœ í•´ì„±ë°©ì§€",
        "score_key": "score",
    },
    # ALT - í¸í–¥ì„±ë°©ì§€
    "kobbq": {
        "category": "ALT_í¸í–¥ì„±ë°©ì§€",
        "score_key": "score",
    },
    # ALT - í™˜ê°ë°©ì§€
    "ko_truthful_qa": {
        "category": "ALT_í™˜ê°ë°©ì§€",
        "score_key": "score",
    },
    "ko_hallulens_wikiqa": {
        "category": "ALT_í™˜ê°ë°©ì§€",
        "score_key": "score",
    },
    "ko_hallulens_longwiki": {
        "category": "ALT_í™˜ê°ë°©ì§€",
        "score_key": "score",
    },
    "ko_hallulens_nonexistent": {
        "category": "ALT_í™˜ê°ë°©ì§€",
        "score_key": "score",
    },
}

# GLP ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
GLP_COLUMN_WEIGHT = {
    "GLP_êµ¬ë¬¸í•´ì„": 1,
    "GLP_ì˜ë¯¸í•´ì„": 1,
    "GLP_í‘œí˜„": 1,
    "GLP_ì •ë³´ê²€ìƒ‰": 1,
    "GLP_ì¼ë°˜ì ì§€ì‹": 2,
    "GLP_ì „ë¬¸ì ì§€ì‹": 2,
    "GLP_ìˆ˜í•™ì ì¶”ë¡ ": 2,
    "GLP_ë…¼ë¦¬ì ì¶”ë¡ ": 2,
    "GLP_ì¶”ìƒì ì¶”ë¡ ": 2,
    "GLP_í•¨ìˆ˜í˜¸ì¶œ": 2,
    "GLP_ì½”ë”©ëŠ¥ë ¥": 2,
}

# ALT ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
ALT_COLUMN_WEIGHT = {
    "ALT_ì œì–´ì„±": 1,
    "ALT_ìœ í•´ì„±ë°©ì§€": 1,
    "ALT_í¸í–¥ì„±ë°©ì§€": 1,
    "ALT_ìœ¤ë¦¬/ë„ë•": 1,
    "ALT_í™˜ê°ë°©ì§€": 1,
}

# GLP ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ â†’ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë ˆì´ë” ì°¨íŠ¸ìš©)
GLP_CATEGORY_MAPPER = {
    "GLP_êµ¬ë¬¸í•´ì„": "ê¸°ë³¸ì–¸ì–´ì„±ëŠ¥",
    "GLP_ì˜ë¯¸í•´ì„": "ê¸°ë³¸ì–¸ì–´ì„±ëŠ¥",
    "GLP_í‘œí˜„": "ì‘ìš©ì–¸ì–´ì„±ëŠ¥",
    "GLP_ì •ë³´ê²€ìƒ‰": "ì‘ìš©ì–¸ì–´ì„±ëŠ¥",
    "GLP_ì¼ë°˜ì ì§€ì‹": "ì§€ì‹/ì§ˆì˜ì‘ë‹µ",
    "GLP_ì „ë¬¸ì ì§€ì‹": "ì§€ì‹/ì§ˆì˜ì‘ë‹µ",
    "GLP_ìˆ˜í•™ì ì¶”ë¡ ": "ì¶”ë¡ ëŠ¥ë ¥",
    "GLP_ë…¼ë¦¬ì ì¶”ë¡ ": "ì¶”ë¡ ëŠ¥ë ¥",
    "GLP_ì¶”ìƒì ì¶”ë¡ ": "ì¶”ë¡ ëŠ¥ë ¥",
    "GLP_í•¨ìˆ˜í˜¸ì¶œ": "ì–´í”Œë¦¬ì¼€ì´ì…˜ê°œë°œ",
    "GLP_ì½”ë”©ëŠ¥ë ¥": "ì–´í”Œë¦¬ì¼€ì´ì…˜ê°œë°œ",
}

# ALT ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ â†’ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë ˆì´ë” ì°¨íŠ¸ìš©)
ALT_CATEGORY_MAPPER = {
    "ALT_ì œì–´ì„±": "ì œì–´ì„±",
    "ALT_ìœ í•´ì„±ë°©ì§€": "ìœ í•´ì„±ë°©ì§€",
    "ALT_í¸í–¥ì„±ë°©ì§€": "í¸í–¥ì„±ë°©ì§€",
    "ALT_ìœ¤ë¦¬/ë„ë•": "ìœ¤ë¦¬/ë„ë•",
    "ALT_í™˜ê°ë°©ì§€": "í™˜ê°ë°©ì§€",
}


def weighted_average(df: pd.DataFrame, weights_dict: dict[str, float]) -> pd.Series:
    """ê°€ì¤‘ í‰ê·  ê³„ì‚°"""
    cols = [c for c in weights_dict.keys() if c in df.columns]
    if not cols:
        return pd.Series([None] * len(df), index=df.index)
    weights = [weights_dict[c] for c in cols]
    return (df[cols].mul(weights, axis=1).sum(axis=1)) / sum(weights)


def create_leaderboard_table(
    model_name: str,
    benchmark_scores: dict[str, dict[str, Any]],
    metadata: dict[str, Any] | None = None,
) -> pd.DataFrame:
    """
    ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ë¥¼ ë¦¬ë”ë³´ë“œ í…Œì´ë¸”ë¡œ ë³€í™˜
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        benchmark_scores: ë²¤ì¹˜ë§ˆí¬ë³„ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
            ì˜ˆ: {"kmmlu": {"score": 0.85, "details": {...}}, ...}
        metadata: ëª¨ë¸ ë©”íƒ€ë°ì´í„° (release_date, size_category, model_size ë“±)
    
    Returns:
        ë¦¬ë”ë³´ë“œ DataFrame
    """
    if metadata is None:
        metadata = {}
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ìˆ˜ì§‘
    category_scores: dict[str, list[float]] = {}
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë²¤ì¹˜ë§ˆí¬ ì¶œë ¥ìš© ìˆ˜ì§‘
    category_benchmarks: dict[str, list[tuple[str, float]]] = {}
    unknown_benchmarks: list[tuple[str, float]] = []
    
    for benchmark_name, score_info in benchmark_scores.items():
        main_score = score_info.get("score")
        
        if benchmark_name not in BENCHMARK_TAXONOMY:
            if main_score is not None:
                unknown_benchmarks.append((benchmark_name, main_score))
            continue
        
        taxonomy = BENCHMARK_TAXONOMY[benchmark_name]
        main_category = taxonomy["category"]
        
        if main_score is not None:
            if main_category not in category_scores:
                category_scores[main_category] = []
            category_scores[main_category].append(main_score)
            
            # ì¶œë ¥ìš© ìˆ˜ì§‘
            if main_category not in category_benchmarks:
                category_benchmarks[main_category] = []
            category_benchmarks[main_category].append((benchmark_name, main_score))
        
        # sub_scoresê°€ ìˆëŠ” ê²½ìš° (ko_mtbench ë“±)
        if "sub_scores" in taxonomy and score_info.get("details"):
            for detail_key, sub_category in taxonomy["sub_scores"].items():
                detail_value = score_info["details"].get(detail_key)
                if detail_value is not None:
                    if sub_category not in category_scores:
                        category_scores[sub_category] = []
                    category_scores[sub_category].append(detail_value)
                    
                    # ì¶œë ¥ìš© ìˆ˜ì§‘
                    if sub_category not in category_benchmarks:
                        category_benchmarks[sub_category] = []
                    category_benchmarks[sub_category].append((f"{benchmark_name}.{detail_key}", detail_value))
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í•‘í•˜ì—¬ ì¶œë ¥
    print(f"   ğŸ“Š Processing {len(benchmark_scores)} benchmarks...")
    
    # GLP ì¹´í…Œê³ ë¦¬ ë¨¼ì €, ALT ì¹´í…Œê³ ë¦¬ ë‚˜ì¤‘ì—
    glp_categories = sorted([c for c in category_benchmarks.keys() if c.startswith("GLP_")])
    alt_categories = sorted([c for c in category_benchmarks.keys() if c.startswith("ALT_")])
    
    for category in glp_categories + alt_categories:
        benchmarks = category_benchmarks[category]
        print(f"\n   ğŸ“‚ {category}")
        for bench_name, score in benchmarks:
            print(f"      âœ… {bench_name}: {score:.4f}")
    
    # ì•Œ ìˆ˜ ì—†ëŠ” ë²¤ì¹˜ë§ˆí¬ ì¶œë ¥
    if unknown_benchmarks:
        print(f"\n   âš ï¸ Unknown benchmarks (not in taxonomy):")
        for bench_name, score in unknown_benchmarks:
            print(f"      - {bench_name}: {score}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ê³„ì‚°
    category_means = {}
    for category, scores in category_scores.items():
        if scores:
            category_means[category] = sum(scores) / len(scores)
    
    print(f"   ğŸ“ˆ Category scores: {len(category_means)} categories")
    for cat, val in category_means.items():
        print(f"      {cat}: {val:.4f}")
    
    # DataFrame ìƒì„±
    row_data = {"model_name": model_name}
    row_data.update(category_means)
    
    df = pd.DataFrame([row_data])
    df.set_index("model_name", inplace=True)
    
    # GLP/ALT í‰ê·  ê³„ì‚°
    df["ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG"] = weighted_average(df, GLP_COLUMN_WEIGHT)
    df["ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG"] = weighted_average(df, ALT_COLUMN_WEIGHT)
    
    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    glp_avg = df["ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG"].iloc[0]
    alt_avg = df["ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG"].iloc[0]
    
    if pd.notna(glp_avg) and pd.notna(alt_avg):
        df["FINAL_SCORE"] = (glp_avg + alt_avg) / 2
    elif pd.notna(glp_avg):
        df["FINAL_SCORE"] = glp_avg
    elif pd.notna(alt_avg):
        df["FINAL_SCORE"] = alt_avg
    else:
        df["FINAL_SCORE"] = None
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    release_date = metadata.get("release_date")
    if release_date and release_date != "unknown":
        try:
            df["release_date"] = pd.to_datetime(release_date, format="%Y-%m-%d")
        except (ValueError, TypeError):
            df["release_date"] = pd.NaT
    else:
        df["release_date"] = pd.NaT
    df["size_category"] = metadata.get("size_category", "unknown")
    df["model_size"] = metadata.get("model_size", "unknown")
    
    df = df.reset_index()
    
    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
    priority_columns = [
        "model_name",
        "release_date",
        "size_category",
        "model_size",
        "FINAL_SCORE",
        "ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG",
        "ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG",
    ]
    glp_columns = [c for c in GLP_COLUMN_WEIGHT.keys() if c in df.columns]
    alt_columns = [c for c in ALT_COLUMN_WEIGHT.keys() if c in df.columns]
    
    ordered_columns = []
    for col in priority_columns + glp_columns + alt_columns:
        if col in df.columns and col not in ordered_columns:
            ordered_columns.append(col)
    
    # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ ì¶”ê°€
    for col in df.columns:
        if col not in ordered_columns:
            ordered_columns.append(col)
    
    return df[ordered_columns]


def create_radar_tables(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    ë ˆì´ë” ì°¨íŠ¸ìš© í…Œì´ë¸” ìƒì„±
    
    Args:
        df: ë¦¬ë”ë³´ë“œ DataFrame
    
    Returns:
        (glp_radar_table, glp_detail_radar_table, alt_radar_table, alt_detail_radar_table)
    """
    # GLP ë ˆì´ë” í…Œì´ë¸”
    glp_cols = [c for c in GLP_CATEGORY_MAPPER.keys() if c in df.columns]
    if glp_cols:
        glp_detail_radar = df[glp_cols].T.reset_index()
        glp_detail_radar.columns = ["category", "score"]
        
        # ìƒìœ„ ì¹´í…Œê³ ë¦¬ë¡œ ê·¸ë£¹í•‘
        glp_detail_radar["group"] = glp_detail_radar["category"].map(GLP_CATEGORY_MAPPER)
        glp_radar = glp_detail_radar.groupby("group")["score"].mean().reset_index()
        glp_radar.columns = ["category", "score"]
    else:
        glp_radar = pd.DataFrame(columns=["category", "score"])
        glp_detail_radar = pd.DataFrame(columns=["category", "score"])
    
    # ALT ë ˆì´ë” í…Œì´ë¸”
    alt_cols = [c for c in ALT_CATEGORY_MAPPER.keys() if c in df.columns]
    if alt_cols:
        alt_detail_radar = df[alt_cols].T.reset_index()
        alt_detail_radar.columns = ["category", "score"]
        
        # ìƒìœ„ ì¹´í…Œê³ ë¦¬ë¡œ ê·¸ë£¹í•‘
        alt_detail_radar["group"] = alt_detail_radar["category"].map(ALT_CATEGORY_MAPPER)
        alt_radar = alt_detail_radar.groupby("group")["score"].mean().reset_index()
        alt_radar.columns = ["category", "score"]
    else:
        alt_radar = pd.DataFrame(columns=["category", "score"])
        alt_detail_radar = pd.DataFrame(columns=["category", "score"])
    
    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
    if "group" in glp_detail_radar.columns:
        glp_detail_radar = glp_detail_radar.drop(columns=["group"])
    if "group" in alt_detail_radar.columns:
        alt_detail_radar = alt_detail_radar.drop(columns=["group"])
    
    return glp_radar, glp_detail_radar, alt_radar, alt_detail_radar


def create_benchmark_detail_table(
    model_name: str,
    benchmark_scores: dict[str, dict[str, Any]],
) -> pd.DataFrame:
    """
    ë²¤ì¹˜ë§ˆí¬ë³„ ìƒì„¸ ì ìˆ˜ í…Œì´ë¸” ìƒì„±
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        benchmark_scores: ë²¤ì¹˜ë§ˆí¬ë³„ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ë²¤ì¹˜ë§ˆí¬ë³„ ìƒì„¸ DataFrame
    """
    rows = []
    
    for benchmark_name, score_info in benchmark_scores.items():
        row = {
            "model_name": model_name,
            "benchmark": benchmark_name,
            "score": score_info.get("score"),
        }
        
        # íƒì†Œë…¸ë¯¸ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        if benchmark_name in BENCHMARK_TAXONOMY:
            row["category"] = BENCHMARK_TAXONOMY[benchmark_name]["category"]
        else:
            row["category"] = "ê¸°íƒ€"
        
        # ìƒì„¸ ì ìˆ˜ ì¶”ê°€
        details = score_info.get("details", {})
        for key, value in details.items():
            row[f"detail_{key}"] = value
        
        rows.append(row)
    
    return pd.DataFrame(rows)


def log_leaderboard_tables(
    wandb_run: wandb.sdk.wandb_run.Run,
    model_name: str,
    benchmark_scores: dict[str, dict[str, Any]],
    metadata: dict[str, Any] | None = None,
) -> None:
    """
    W&Bì— ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ë¡œê¹…
    
    Args:
        wandb_run: W&B run ê°ì²´
        model_name: ëª¨ë¸ ì´ë¦„
        benchmark_scores: ë²¤ì¹˜ë§ˆí¬ë³„ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
        metadata: ëª¨ë¸ ë©”íƒ€ë°ì´í„°
    """
    if not benchmark_scores:
        print("âš ï¸ No benchmark scores to log")
        return
    
    # ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ìƒì„±
    leaderboard_df = create_leaderboard_table(model_name, benchmark_scores, metadata)
    
    # ë ˆì´ë” ì°¨íŠ¸ í…Œì´ë¸” ìƒì„±
    glp_radar, glp_detail_radar, alt_radar, alt_detail_radar = create_radar_tables(leaderboard_df)
    
    # ë²¤ì¹˜ë§ˆí¬ë³„ ìƒì„¸ í…Œì´ë¸” ìƒì„±
    benchmark_detail_df = create_benchmark_detail_table(model_name, benchmark_scores)
    
    # W&B Tableë¡œ ë³€í™˜ ë° ë¡œê¹…
    try:
        wandb_run.log({"leaderboard_table": wandb.Table(dataframe=leaderboard_df)})
        print("âœ… Logged: leaderboard_table")
        
        wandb_run.log({"benchmark_detail_table": wandb.Table(dataframe=benchmark_detail_df)})
        print("âœ… Logged: benchmark_detail_table")
        
        if not glp_radar.empty:
            wandb_run.log({"glp_radar_table": wandb.Table(dataframe=glp_radar)})
            print("âœ… Logged: glp_radar_table")
        
        if not glp_detail_radar.empty:
            wandb_run.log({"glp_detail_radar_table": wandb.Table(dataframe=glp_detail_radar)})
            print("âœ… Logged: glp_detail_radar_table")
        
        if not alt_radar.empty:
            wandb_run.log({"alt_radar_table": wandb.Table(dataframe=alt_radar)})
            print("âœ… Logged: alt_radar_table")
        
        if not alt_detail_radar.empty:
            wandb_run.log({"alt_detail_radar_table": wandb.Table(dataframe=alt_detail_radar)})
            print("âœ… Logged: alt_detail_radar_table")
        
        # Summaryì— ì ìˆ˜ ì¶”ê°€
        # 1) leaderboard_tableì˜ ëª¨ë“  ì»¬ëŸ¼ì„ summaryì— ì €ì¥ (ì¹´í…Œê³ ë¦¬ ìŠ¤ì½”ì–´ + ë©”íƒ€ë°ì´í„°)
        for col in leaderboard_df.columns:
            if col == "model_name":
                continue
            value = leaderboard_df[col].iloc[0]
            if pd.notna(value):
                wandb_run.summary[col] = value
        
        # ê¸°ì¡´ í‚¤ í˜¸í™˜ì„± ìœ ì§€ (final_score, glp_avg, alt_avg)
        if "FINAL_SCORE" in leaderboard_df.columns:
            final_score = leaderboard_df["FINAL_SCORE"].iloc[0]
            if pd.notna(final_score):
                wandb_run.summary["final_score"] = final_score
        
        if "ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG" in leaderboard_df.columns:
            glp_avg = leaderboard_df["ë²”ìš©ì–¸ì–´ì„±ëŠ¥(GLP)_AVG"].iloc[0]
            if pd.notna(glp_avg):
                wandb_run.summary["glp_avg"] = glp_avg
        
        if "ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG" in leaderboard_df.columns:
            alt_avg = leaderboard_df["ê°€ì¹˜ì •ë ¬ì„±ëŠ¥(ALT)_AVG"].iloc[0]
            if pd.notna(alt_avg):
                wandb_run.summary["alt_avg"] = alt_avg
        
        # 2) benchmark_detail_tableì—ì„œ ê°œë³„ ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ ì €ì¥
        for _, row in benchmark_detail_df.iterrows():
            benchmark_name = row.get("benchmark")
            score = row.get("score")
            if benchmark_name and pd.notna(score):
                wandb_run.summary[f"benchmark/{benchmark_name}"] = score
        
        print("âœ… W&B Leaderboard tables logged successfully!")
        
    except Exception as e:
        print(f"âŒ Failed to log W&B tables: {e}")
        raise

