# Horangi ê°œë°œ ê°€ì´ë“œ

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
src/horangi/
â”œâ”€â”€ evals/              # ë²¤ì¹˜ë§ˆí¬ ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ __init__.py     # ë²¤ì¹˜ë§ˆí¬ ë“±ë¡
â”‚   â”œâ”€â”€ ko_hellaswag.py
â”‚   â”œâ”€â”€ kmmlu.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ core/               # í•µì‹¬ ë¡œì§
â”‚   â”œâ”€â”€ factory.py      # Task ìƒì„± (create_benchmark)
â”‚   â”œâ”€â”€ loaders.py      # ë°ì´í„° ë¡œë”© (Weave, JSONL)
â”‚   â””â”€â”€ answer_format.py # ì •ë‹µ í˜•ì‹ ë³€í™˜
â”œâ”€â”€ scorers/            # ì»¤ìŠ¤í…€ Scorer
â”‚   â”œâ”€â”€ __init__.py     # Scorer ë“±ë¡
â”‚   â”œâ”€â”€ bfcl_scorer.py
â”‚   â”œâ”€â”€ kobbq_scorer.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ solvers/            # ì»¤ìŠ¤í…€ Solver
â”‚   â”œâ”€â”€ __init__.py     # Solver ë“±ë¡
â”‚   â””â”€â”€ bfcl_solver.py
â””â”€â”€ data/               # ë¡œì»¬ ë°ì´í„° íŒŒì¼ (JSONL)
```

---

## ğŸ¯ ìƒˆ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€í•˜ê¸°

### Step 1: Config íŒŒì¼ ìƒì„±

`evals/` í´ë”ì— ìƒˆ íŒŒì¼ì„ ë§Œë“¤ê³  `CONFIG` ë”•ì…”ë„ˆë¦¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

```python
# evals/my_benchmark.py
"""
My Benchmark - ë²¤ì¹˜ë§ˆí¬ ì„¤ëª…

ì›ë³¸: [ë§í¬]
ë°ì´í„°: Weave ë˜ëŠ” JSONL
"""

CONFIG = {
    # ë°ì´í„° ì†ŒìŠ¤
    "data_type": "weave",  # "weave" ë˜ëŠ” "jsonl"
    "data_source": "weave:///entity/project/object/DatasetName:latest",
    
    # í•„ë“œ ë§¤í•‘
    "field_mapping": {
        "id": "id",           # ìƒ˜í”Œ ID
        "input": "question",  # ì…ë ¥ (ì§ˆë¬¸)
        "target": "answer",   # ì •ë‹µ (MCQA: A/B/C/D, ìƒì„±: í…ìŠ¤íŠ¸)
        "choices": "options", # ì„ íƒì§€ (MCQAë§Œ)
    },
    
    # ì •ë‹µ í˜•ì‹ ë³€í™˜
    "answer_format": "identity",  # ì•„ë˜ ì˜µì…˜ ì°¸ê³ 
    
    # Solver & Scorer
    "solver": "multiple_choice",  # ë˜ëŠ” "generate"
    "scorer": "choice",           # ë˜ëŠ” "match", ì»¤ìŠ¤í…€ scorer
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    "system_message": "ì£¼ì–´ì§„ ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ë‹µì„ ì„ íƒí•˜ì„¸ìš”.",
}
```

### Step 2: `evals/__init__.py`ì— ë“±ë¡

```python
# evals/__init__.pyì— ì¶”ê°€
from horangi.evals.my_benchmark import CONFIG as my_benchmark

BENCHMARKS: dict = {
    ...
    "my_benchmark": my_benchmark,
}
```

### Step 3: `eval_tasks.py`ì— Task í•¨ìˆ˜ ì¶”ê°€

```python
# eval_tasks.pyì— ì¶”ê°€
@task
def my_benchmark(shuffle: bool = False, limit: int | None = None) -> Task:
    """My Benchmark - ì„¤ëª…"""
    return create_benchmark(name="my_benchmark", shuffle=shuffle, limit=limit)
```

---

## ğŸ“‹ Config í•„ë“œ ìƒì„¸ ì„¤ëª…

### `data_type` & `data_source`

| data_type | data_source í˜•ì‹ | ì˜ˆì‹œ |
|-----------|------------------|------|
| `weave` | Weave ê°ì²´ URI | `weave:///wandb-korea/evaluation-job/object/KMMLU:latest` |
| `jsonl` | íŒŒì¼ëª… (data/ ê¸°ì¤€) | `ko_aime2025.jsonl` |

### `field_mapping`

ë°ì´í„°ì…‹ í•„ë“œ â†’ Sample í•„ë“œ ë§¤í•‘

| Sample í•„ë“œ | ì„¤ëª… | í•„ìˆ˜ |
|-------------|------|------|
| `id` | ìƒ˜í”Œ ê³ ìœ  ID | âŒ |
| `input` | ëª¨ë¸ ì…ë ¥ (ì§ˆë¬¸) | âœ… |
| `target` | ì •ë‹µ | âŒ (ê±°ë¶€ íƒœìŠ¤í¬ ë“±) |
| `choices` | ì„ íƒì§€ ë¦¬ìŠ¤íŠ¸ | âŒ (MCQAë§Œ) |

**ì—¬ëŸ¬ í•„ë“œ í›„ë³´ ì§€ì •:**
```python
"id": ["id", "sample_id", "idx"],  # ìˆœì„œëŒ€ë¡œ ì‹œë„
```

### `answer_format`

ì •ë‹µ ë³€í™˜ ë°©ì‹:

| ê°’ | ì„¤ëª… | ì˜ˆì‹œ |
|----|------|------|
| `identity` | ë³€í™˜ ì—†ìŒ | `"ì •ë‹µ"` â†’ `"ì •ë‹µ"` |
| `index_0` | 0-indexed ìˆ«ì â†’ A/B/C | `0` â†’ `"A"` |
| `to_string` | ìˆ«ì â†’ ë¬¸ìì—´ | `42` â†’ `"42"` |
| `text` | í…ìŠ¤íŠ¸ â†’ ì„ íƒì§€ ì¸ë±ìŠ¤ | `"ì‚¬ê³¼"` â†’ `"A"` (choices í•„ìš”) |

### `solver`

| ê°’ | ì„¤ëª… | ìš©ë„ |
|----|------|------|
| `multiple_choice` | ì„ íƒì§€ ì œì‹œ + ì„ íƒ | MCQA |
| `generate` | ììœ  í˜•ì‹ ìƒì„± | ìƒì„± íƒœìŠ¤í¬ |
| `bfcl_solver` | Tool calling | BFCL |
| `bfcl_text_solver` | í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ í•¨ìˆ˜ í˜¸ì¶œ | BFCL (ì˜¤í”ˆì†ŒìŠ¤) |

### `scorer`

| ê°’ | ì„¤ëª… | ìš©ë„ |
|----|------|------|
| `choice` | ì„ íƒì§€ ì •í™•ë„ | MCQA |
| `match` | ì •í™• ì¼ì¹˜ | ë‹¨ë‹µí˜• |
| `match_numeric` | ìˆ«ì ì¼ì¹˜ | ìˆ˜í•™ |
| `model_graded_qa` | LLM ì±„ì  | ì£¼ê´€ì‹ |
| ì»¤ìŠ¤í…€ | `scorers/`ì— ì •ì˜ | íŠ¹ìˆ˜ í‰ê°€ |

### ì¶”ê°€ ì˜µì…˜

| í•„ë“œ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `base` | inspect_evals ìƒì† | `"inspect_evals.hellaswag.hellaswag"` |
| `split` | ë°ì´í„° ë¶„í•  | `"train"`, `"test"` |
| `sampling` | ìƒ˜í”Œë§ ë°©ì‹ | `"stratified"`, `"balanced"` |
| `sampling_by` | ê·¸ë£¹í™” í•„ë“œ | `"category"` |
| `default_fields` | ëˆ„ë½ í•„ë“œ ê¸°ë³¸ê°’ | `{"image": None}` |

---

## ğŸ”§ ì»¤ìŠ¤í…€ Scorer ì¶”ê°€í•˜ê¸°

### Step 1: Scorer íŒŒì¼ ìƒì„±

```python
# scorers/my_scorer.py
"""
My Custom Scorer - ì„¤ëª…
"""

from inspect_ai.scorer import (
    Score, Scorer, Target, scorer, metric, Metric,
    SampleScore, accuracy, CORRECT, INCORRECT,
)
from inspect_ai.solver import TaskState


@metric
def my_custom_metric() -> Metric:
    """ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­"""
    def metric_fn(scores: list[SampleScore]) -> float:
        # ì ìˆ˜ ê³„ì‚° ë¡œì§
        correct = sum(1 for s in scores if s.score.value == CORRECT)
        return correct / len(scores) if scores else 0.0
    return metric_fn


@scorer(metrics=[accuracy(), my_custom_metric()])
def my_scorer() -> Scorer:
    """ì»¤ìŠ¤í…€ Scorer"""
    async def score(state: TaskState, target: Target) -> Score:
        response = state.output.completion
        expected = target.text
        
        # í‰ê°€ ë¡œì§
        is_correct = response.strip() == expected.strip()
        
        return Score(
            value=CORRECT if is_correct else INCORRECT,
            answer=response[:100],
            explanation=f"Expected: {expected}, Got: {response[:50]}",
            metadata={"custom_field": "value"},
        )
    
    return score
```

### Step 2: `scorers/__init__.py`ì— ë“±ë¡

```python
from horangi.scorers.my_scorer import my_scorer

__all__ = [
    ...
    "my_scorer",
]
```

### Step 3: Configì—ì„œ ì‚¬ìš©

```python
CONFIG = {
    ...
    "scorer": "my_scorer",
}
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

ìƒˆ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€ ì‹œ í™•ì¸ì‚¬í•­:

- [ ] `evals/` í´ë”ì— config íŒŒì¼ ìƒì„±
- [ ] `evals/__init__.py`ì— import ë° BENCHMARKS ì¶”ê°€
- [ ] `eval_tasks.py`ì— @task í•¨ìˆ˜ ì¶”ê°€
- [ ] (ì»¤ìŠ¤í…€ scorer í•„ìš” ì‹œ) `scorers/`ì— íŒŒì¼ ìƒì„± ë° ë“±ë¡
- [ ] (ì»¤ìŠ¤í…€ solver í•„ìš” ì‹œ) `solvers/`ì— íŒŒì¼ ìƒì„± ë° ë“±ë¡
- [ ] í…ŒìŠ¤íŠ¸ ì‹¤í–‰: `inspect eval eval_tasks.py@my_benchmark --model openai/gpt-4o -T limit=5`

---

## ğŸ”— ì°¸ê³ 

- [Inspect AI Docs](https://inspect.ai-safety-institute.org.uk/)
- [inspect_evals GitHub](https://github.com/UKGovernmentBEIS/inspect_evals)
- [WandB Weave](https://wandb.ai/site/weave)

