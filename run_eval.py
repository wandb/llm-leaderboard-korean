#!/usr/bin/env python3
"""
í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” inspect-wandbë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³ 
ê²°ê³¼ë¥¼ WandB/Weaveì— ìë™ìœ¼ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    python run_eval.py --model openai/gpt-4o

    # íŠ¹ì • ë²¤ì¹˜ë§ˆí¬ë§Œ ì‹¤í–‰
    python run_eval.py --model openai/gpt-4o --benchmark korean_qa

    # WandB í”„ë¡œì íŠ¸ ì§€ì •
    python run_eval.py --model anthropic/claude-sonnet-4-0 --wandb-project my-korean-eval
"""

import argparse
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))


def setup_wandb(project: str, entity: str | None = None):
    """WandB ì„¤ì • ì´ˆê¸°í™”"""
    os.environ.setdefault("WANDB_PROJECT", project)
    if entity:
        os.environ["WANDB_ENTITY"] = entity

    # inspect-wandbê°€ ìë™ìœ¼ë¡œ wandb/weave ë¡œê¹…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤
    print(f"WandB í”„ë¡œì íŠ¸: {project}")
    if entity:
        print(f"WandB ì—”í‹°í‹°: {entity}")


def run_benchmark(
    benchmark: str,
    model: str,
    use_cot: bool = False,
    limit: int | None = None,
):
    """
    ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

    Args:
        benchmark: ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ (korean_qa, korean_reasoning, korean_knowledge, korean_commonsense)
        model: í‰ê°€í•  ëª¨ë¸ (ì˜ˆ: openai/gpt-4o, anthropic/claude-sonnet-4-0)
        use_cot: Chain-of-thought ì‚¬ìš© ì—¬ë¶€
        limit: í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
    """
    from inspect_ai import eval

    # ë²¤ì¹˜ë§ˆí¬ Task ì„í¬íŠ¸
    if benchmark == "korean_qa":
        from horangi.benchmarks import korean_qa
        task = korean_qa(use_cot=use_cot)
    elif benchmark == "korean_reasoning":
        from horangi.benchmarks import korean_reasoning
        task = korean_reasoning(use_cot=use_cot)
    elif benchmark == "korean_knowledge":
        from horangi.benchmarks import korean_knowledge
        task = korean_knowledge()
    elif benchmark == "korean_commonsense":
        from horangi.benchmarks import korean_commonsense
        task = korean_commonsense(use_cot=use_cot)
    else:
        raise ValueError(f"Unknown benchmark: {benchmark}")

    print(f"\n{'='*60}")
    print(f"ë²¤ì¹˜ë§ˆí¬: {benchmark}")
    print(f"ëª¨ë¸: {model}")
    print(f"Chain-of-Thought: {'í™œì„±í™”' if use_cot else 'ë¹„í™œì„±í™”'}")
    print(f"{'='*60}\n")

    # í‰ê°€ ì‹¤í–‰
    eval_args = {
        "model": model,
    }
    
    if limit:
        eval_args["limit"] = limit

    results = eval(task, **eval_args)
    
    return results


def run_all_benchmarks(
    model: str,
    use_cot: bool = False,
    limit: int | None = None,
):
    """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    benchmarks = [
        "korean_qa",
        "korean_reasoning",
        "korean_knowledge",
        "korean_commonsense",
    ]

    all_results = {}
    for benchmark in benchmarks:
        print(f"\n[{benchmarks.index(benchmark) + 1}/{len(benchmarks)}] {benchmark} í‰ê°€ ì¤‘...")
        try:
            results = run_benchmark(benchmark, model, use_cot, limit)
            all_results[benchmark] = results
        except Exception as e:
            print(f"âš ï¸ {benchmark} í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            all_results[benchmark] = None

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    for benchmark, results in all_results.items():
        if results:
            print(f"  {benchmark}: ì™„ë£Œ")
        else:
            print(f"  {benchmark}: ì‹¤íŒ¨")

    return all_results


def main():
    parser = argparse.ArgumentParser(
        description="í•œêµ­ì–´ LLM ë²¤ì¹˜ë§ˆí¬ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # GPT-4oë¡œ í•œêµ­ì–´ QA í‰ê°€
  python run_eval.py --model openai/gpt-4o --benchmark korean_qa

  # Claudeë¡œ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ í‰ê°€
  python run_eval.py --model anthropic/claude-sonnet-4-0

  # Chain-of-Thought í™œì„±í™”
  python run_eval.py --model openai/gpt-4o --benchmark korean_reasoning --cot

  # ìƒ˜í”Œ ìˆ˜ ì œí•œí•˜ì—¬ í…ŒìŠ¤íŠ¸
  python run_eval.py --model openai/gpt-4o --limit 5
        """
    )
    
    parser.add_argument(
        "--model", "-m",
        required=True,
        help="í‰ê°€í•  ëª¨ë¸ (ì˜ˆ: openai/gpt-4o, anthropic/claude-sonnet-4-0)",
    )
    parser.add_argument(
        "--benchmark", "-b",
        choices=["korean_qa", "korean_reasoning", "korean_knowledge", "korean_commonsense", "all"],
        default="all",
        help="ì‹¤í–‰í•  ë²¤ì¹˜ë§ˆí¬ (ê¸°ë³¸: all)",
    )
    parser.add_argument(
        "--cot",
        action="store_true",
        help="Chain-of-Thought í”„ë¡¬í”„íŒ… í™œì„±í™”",
    )
    parser.add_argument(
        "--limit", "-l",
        type=int,
        default=None,
        help="í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)",
    )
    parser.add_argument(
        "--wandb-project", "-p",
        default="korean-llm-benchmark",
        help="WandB í”„ë¡œì íŠ¸ ì´ë¦„ (ê¸°ë³¸: korean-llm-benchmark)",
    )
    parser.add_argument(
        "--wandb-entity", "-e",
        default=None,
        help="WandB ì—”í‹°í‹° (íŒ€ ë˜ëŠ” ì‚¬ìš©ì ì´ë¦„)",
    )

    args = parser.parse_args()

    # WandB ì„¤ì •
    setup_wandb(args.wandb_project, args.wandb_entity)

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    if args.benchmark == "all":
        results = run_all_benchmarks(args.model, args.cot, args.limit)
    else:
        results = run_benchmark(args.benchmark, args.model, args.cot, args.limit)

    print("\nâœ… í‰ê°€ ì™„ë£Œ!")
    print("ğŸ“Š WandB ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()

