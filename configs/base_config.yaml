# wandb configurations
wandb:
  params:
    entity: horangi
    project: horangi4-dev
    project_dataset: horangi4-dataset

# testmode configurations
testmode: false

# dataset configurations

komoral:
  split: test
  subset: default
  limit: 100
  evaluation:
    method: string_match

mt_bench:
  split: test
  subset: default
  limit: 100
  evaluation:
    method: "mt_bench_judge"
    params:
      judge_backend_name: openai_judge
      model_name: gpt-4.1-2025-04-14
      api_base: https://api.openai.com/v1
      batch_size: 10
      max_tokens: 512
      temperature: 0.0

kobbq:
  split: test
  subset: default
  limit: 1000
  evaluation:
    method: string_match

squad_kor_v1:
  split: test
  subset: default
  limit: 1000
  evaluation:
    method: char_f1

ifeval_ko:
  split: test
  subset: default
  limit: 100
  evaluation:
    method: ifeval_strict

mrcr_2_needles:
  split: train
  subset: 128k
  evaluation:
    method: sequence_match
    params:
      prefix_key: random_string_to_prepend
      require_prefix: true
      strip_prefix: true

haerae_bench_v1:
  subset: [standard_nomenclature, loan_words, rare_words, general_knowledge, history, reading_comprehension]
  split: test
  evaluation:
    method: string_match
    params:
      mcqa: true

korean_parallel_corpora:
  subset: [e2k, k2e]
  split: test
  limit: 100
  evaluation:
    method: comet_score

korean_hate_speech:
  split: train
  subset: default
  limit: 1000
  evaluation:
    method: string_match

kmmlu:
  split: test
  limit: 100
  evaluation:
    method: "string_match"
    params:
      mcqa: true
    
kmmlu_pro:
  split: test
  limit: 100
  evaluation:
    method: "string_match"
    params:
      mcqa: true

kmmlu_hard:
  split: test
  limit: 10
  evaluation:
    method: "string_match"
    params:
      mcqa: true

kobalt_700:
  split: test
  limit: 1000
  subset: ["Syntax", "Semantics"] #, "Pragmatics", "Phonetics/Phonology", "Morphology"]
  evaluation:
    method: "string_match"
    params:
      mcqa: true

aime2025:
  split: test
  limit: 100
  subset: ["AIME2025-I", "AIME2025-II"]
  evaluation:
    method: "math_match"
    params:
      extract_final_answer: true # 답변에서 최종 수식/결과를 추출합니다.

hrm8k:
  split: test
  subset: ["GSM8K", "KSM", "MATH", "MMMLU", "OMNI_MATH"]
  limit: 10
  evaluation:
    method: "math_match"
    params:
      extract_final_answer: true # 답변에서 최종 수식/결과를 추출합니다.

hle:
  subset: ["Other", "Humanities/Social Science", "Math", "Physics", "Computer Science/AI", "Biology/Medicine", "Chemistry", "Engineering"]
  split: test
  limit: 1000
  evaluation:
    method: "string_match"

arc_agi:
  split: evaluation
  subset: default
  limit: 1000
  evaluation:
    method: "grid_match"

halluLens:
  split: test
  subset: [precise_wikiqa, mixed_entities, generated_entities]
  limit: 1000
  evaluation:
    abstention:
      model: gpt-4o-mini
    hallucination:
      model: gpt-4o

# SWE-bench Verified 설정
swe_bench_verified:
  split: test
  subset: official_80
  limit: 80
  evaluation:
    method: "swebench_verified"
  # 원격 SWE-bench API 서버 설정 (있을 경우 실제 실행)
  server:
    url: ""  # 예: https://swebench-api.my-domain.com
    token: ""  # 필요 시 토큰
    poll_interval_sec: 10
    max_wait_sec: 36000

bfcl:
  split: test
  subset: [
    simple_python,
    simple_java,
    simple_javascript,
    multiple,
    irrelevance,
    live_simple,
    live_multiple,
    live_irrelevance,
    live_relevance,
    multi_turn_base,
    multi_turn_miss_func,
    multi_turn_miss_param,
  ]
  limit: 1000
  # 동시성 제어: BFCL 모델 inference 시 동시에 보낼 스레드 개수
  num_threads: 32
  evaluation:
    model: gpt-4o-2024-11-20

# arc_agi2:
#   split: evaluation
#   subset: default
#   limit: 1000
#   evaluation:
#     method: "grid_match"