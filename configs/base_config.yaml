wandb:
  entity: "wandb-korea"
  project: "llm-leaderboard3"
  #run: please set up run name in a model-base config

github_version: v3.0.0 #for recording

testmode: false
inference_interval: 0 # seconds

run:
  kaster: true
  haerae_bench_v1: true
  kmmlu_robustness: true # if this is set as true, kaster should set as true
  mtbench: true
  kobbq: true
  ko_truthful_qa: true
  # add other evaluation framework here
  aggregate: true

model:
  artifact_path: null
  max_model_len: 3000
  chat_template: null
  dtype: 'float16'
  trust_remote_code: true
  device_map: "auto"
  load_in_8bit: false
  load_in_4bit: false

generator:
  top_p: 1.0
  temperature: 0.1
  max_tokens: 128

num_few_shots: 2

kaster:
  message_intro: "<prompt>"
  artifacts_path: "/workspace/llm-leaderboard-korean/artifacts/eval_data:v0"
  dataset_dir: "kaster"

haerae_bench_v1:
  message_intro: "<prompt>"
  artifacts_path: "/workspace/llm-leaderboard-korean/artifacts/eval_data:v0"
  dataset_dir: "haerae_bench_v1"

kobbq:
  message_intro: "<prompt>"
  artifacts_path: "/workspace/llm-leaderboard-korean/artifacts/eval_data:v0"
  dataset_dir: "kobbq"

ko_truthful_qa:
  message_intro: "<prompt>"
  artifacts_path: "/workspace/llm-leaderboard-korean/artifacts/eval_data:v0"
  dataset_dir: "ko_truthful_qa"

mtbench:
  temperature_override:
    writing: 0.7
    roleplay: 0.7
    extraction: 0.0
    math: 0.0
    coding: 0.0
    reasoning: 0.0
    stem: 0.1
    humanities": 0.1
  question_artifacts_path: "/workspace/llm-leaderboard-korean/artifacts/mtbench_ko_question:v0" # if testmode is true, small dataset will be used
  question_artifacts_path_test: "/workspace/llm-leaderboard-korean/artifacts/mtbench_ko_question:v0"
  referenceanswer_artifacts_path: "/workspace/llm-leaderboard-korean/artifacts/mtbench_ko_referenceanswer:v0" # if testmode is true, small dataset will be used
  referenceanswer_artifacts_path_test: "/workspace/llm-leaderboard-korean/artifacts/mtbench_ko_referenceanswer:v0"
  judge_prompt_artifacts_path: "/workspace/llm-leaderboard-korean/artifacts/mtbench_ko_prompt:v0"
  bench_name: 'korean_mt_bench'
  model_id: null # cannot use '<', '>', ':', '"', '/', '\\', '|', '?', '*', '.'
  question_begin: null 
  question_end: null 
  max_new_token: 1024
  num_choices: 1
  num_gpus_per_model: 1
  num_gpus_total: 1
  max_gpu_memory: null
  dtype: bfloat16 # None or float32 or float16 or bfloat16
  # for gen_judgment
  judge_model: "gpt-4o-2024-05-13"
  mode: 'single'
  baseline_model: null 
  parallel: 80
  first_n: null

sample_dataset:
  artifacts_path: "your artifact path here"
  # add necessary configration here