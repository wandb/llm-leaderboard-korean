# wandb configurations
wandb:
  params:
    entity: horangi
    project: horangi4-dev
    project_dataset: horangi4-dataset
  run_name: swebench-o4mini-2

# testmode configurations
testmode: false

# LLM 호출 간 간격(초). 누락 시 오류 방지를 위해 기본값 제공
inference_interval: 0.0

mt_bench:
  split: test
  subset: ["roleplay", "humanities", "writing", "reasoning", "coding"]
  limit: 100
  evaluation:
    method: "mt_bench_judge"
    params:
      judge_backend_name: openai_judge
      model_name: gpt-4.1-2025-04-14
      api_base: https://api.openai.com/v1
      batch_size: 10
      max_tokens: 512
      temperature: 0.0
# dataset configurations
# SWE-bench Verified 설정


komoral:
  split: test
  subset: default
  limit: 100
  evaluation:
    method: string_match


kobbq:
  split: test
  subset: default
  limit: 100
  evaluation:
    method: string_match

squad_kor_v1:
  split: test
  subset: default
  limit: 100
  evaluation:
    method: char_f1

ifeval_ko:
  split: test
  subset: default
  limit: 100
  evaluation:
    method: ifeval_strict

mrcr_2_needles:
  split: train
  subset: 128k
  evaluation:
    method: sequence_match
    params:
      prefix_key: random_string_to_prepend
      require_prefix: true
      strip_prefix: true
  # 모델 파라미터 오버라이드: Anthropic 레이트리밋 회피용
  model_params:
    # LiteLLM 백엔드 파라미터 기준
    batch_size: 2           # 동시성 축소
    request_delay_sec: 0.3  # 고정 지연(초)
    request_jitter_sec: 0.5 # 추가 지터(0~이 값 사이 균등)
    retry_max: 4
    retry_base_delay: 1.5

haerae_bench_v1:
  subset: [standard_nomenclature, loan_words, rare_words, general_knowledge, history, reading_comprehension]
  limit: 20
  split: test
  evaluation:
    method: string_match
    params:
      mcqa: true

korean_parallel_corpora:
  subset: [e2k, k2e]
  split: test
  limit: 50
  evaluation:
    method: comet_score

korean_hate_speech:
  split: train
  subset: default
  limit: 100
  evaluation:
    method: string_match

kmmlu:
  split: test
  limit: 3
  evaluation:
    method: "string_match"
    params:
      mcqa: true
    
kmmlu_pro:
  split: test
  limit: 2
  evaluation:
    method: "string_match"
    params:
      mcqa: true

# kmmlu_hard:
#   split: test
#   limit: 10
#   evaluation:
#     method: "string_match"
#     params:
#       mcqa: true

kobalt_700:
  split: test
  limit: 100
  subset: ["Syntax", "Semantics"] #, "Pragmatics", "Phonetics/Phonology", "Morphology"]
  evaluation:
    method: "string_match"
    params:
      mcqa: true

aime2025:
  split: test
  limit: 50
  subset: ["AIME2025-I", "AIME2025-II"]
  evaluation:
    method: "math_match"
    params:
      extract_final_answer: true # 답변에서 최종 수식/결과를 추출합니다.

hrm8k:
  split: test
  subset: ["GSM8K", "KSM", "MATH", "MMMLU", "OMNI_MATH"]
  limit: 20
  evaluation:
    method: "math_match"
    params:
      extract_final_answer: true # 답변에서 최종 수식/결과를 추출합니다.

hle:
  subset: ["Other", "Humanities/Social Science", "Math", "Physics", "Computer Science/AI", "Biology/Medicine", "Chemistry", "Engineering"]
  split: test
  limit: 13
  evaluation:
    method: "string_match"

arc_agi:
  split: evaluation
  subset: default
  limit: 100
  evaluation:
    method: "grid_match"

halluLens:
  split: test
  subset: [precise_wikiqa, mixed_entities, generated_entities]
  limit: 100
  evaluation:
    abstention:
      model: gpt-4o-mini
    hallucination:
      model: gpt-4o

bfcl:
  split: test
  subset: [
    simple_python,
    simple_java,
    simple_javascript,
    multiple,
    irrelevance,
    live_simple,
    live_multiple,
    live_irrelevance,
    live_relevance,
    multi_turn_base,
    multi_turn_miss_func,
    multi_turn_miss_param,
  ]
  limit: 10
  # 동시성 제어: BFCL 모델 inference 시 동시에 보낼 스레드 개수
  num_threads: 32
  evaluation:
    model: gpt-4o-2024-11-20

# ================= SWE-bench (integrated) =================
swebench:
  max_samples: 80
  max_workers: 2
  background_eval: false
  fc_enabled: true
  prebuild_images: false
  images:
    namespace: swebench
    tag: latest
  api_server:
    enabled: true
    endpoint: https://api.nejumi-swebench.org/
    # endpoint: http://localhost:8000
    timeout_sec: 1800
    concurrency: 2
  # artifacts_path: llm-leaderboard/nejumi-leaderboard4/swebench_verified_official_80:v0
  # dataset_dir: swebench_verified_official/
  artifacts_path: horangi/horangi4-dataset/swebench_verified_official_80:v4
  dataset_dir: .

# Global LLM settings for SWE-bench run
api: openai_responses
model:
  pretrained_model_name_or_path: gpt-4o #gpt-5-2025-08-07
generator:
  # Responses API는 max_output_tokens를 사용
  max_output_tokens: 90000
  # extra_body:
  #   reasoning:
  #     effort: "high"
  #     summary: "auto"

network:
  http_timeout:
    connect: 10
    read: 300
    write: 300
    pool: 30

# arc_agi2:
#   split: evaluation
#   subset: default
#   limit: 1000
#   evaluation:
#     method: "grid_match"