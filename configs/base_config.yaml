# wandb configurations
wandb:
  params:
    entity: horangi
    project: horangi4-dev
    project_dataset: horangi4-dataset
  run_name: swebench-o4mini-2

# testmode configurations
testmode: true

# LLM 호출 간 간격(초). 누락 시 오류 방지를 위해 기본값 제공
inference_interval: 0.0

mt_bench:
  split: test
  subset: ["roleplay", "humanities", "writing", "reasoning", "coding"]
  params:
    num_samples: 10 # 각 서브셋당 최대 10개의 샘플만 존재
    limit: 2
  evaluation:
    method: "mt_bench_judge"
    params:
      judge_backend_name: openai_judge
      model_name: gpt-4.1-2025-04-14
      api_base: https://api.openai.com/v1
      batch_size: 10
      max_tokens: 512
      temperature: 0.0

komoral:
  split: test
  subset: default
  params:
    num_samples: 100
    limit: 2
  evaluation:
    method: string_match

kobbq:
  split: test
  subset: default
  params:
    num_samples: 100
    limit: 2
  evaluation:
    method: string_match

squad_kor_v1:
  split: test
  subset: default
  params:
    num_samples: 100
    limit: 2
  evaluation:
    method: char_f1

ifeval_ko:
  split: test
  subset: default
  params:
    num_samples: 100
    limit: 2
  evaluation:
    method: ifeval_strict

mrcr_2_needles:
  split: train
  subset: 128k
  params:
    num_samples: 100
    limit: 2
  evaluation:
    method: sequence_match
    params:
      prefix_key: random_string_to_prepend
      require_prefix: true
      strip_prefix: true
  # 모델 파라미터 오버라이드: Anthropic 레이트리밋 회피용
  model_params:
    # LiteLLM 백엔드 파라미터 기준
    batch_size: 2           # 동시성 축소
    request_delay_sec: 0.3  # 고정 지연(초)
    request_jitter_sec: 0.5 # 추가 지터(0~이 값 사이 균등)
    retry_max: 4
    retry_base_delay: 1.5

haerae_bench_v1:
  subset: [standard_nomenclature, loan_words, rare_words, general_knowledge, history, reading_comprehension]
  params:
    num_samples: 20 # RC 100개, 나머지 20개 씩 총 100개
    limit: 2
  split: test
  evaluation:
    method: string_match
    params:
      mcqa: true

korean_parallel_corpora:
  subset: [e2k, k2e]
  split: test
  params:
    num_samples: 50
    limit: 2
  evaluation:
    method: comet_score

korean_hate_speech:
  split: train
  subset: default
  params:
    num_samples: 100
    limit: 2
  evaluation:
    method: string_match

kmmlu:
  split: test
  params:
    num_samples: 3 # 45개 카테고리
    limit: 1
  evaluation:
    method: "string_match"
    params:
      mcqa: true
    
kmmlu_pro:
  split: test
  params:
    num_samples: 2 # 61개 카테고리
    limit: 1
  evaluation:
    method: "string_match"
    params:
      mcqa: true

# kmmlu_hard:
#   split: test
#   num_samples: 10
#   evaluation:
#     method: "string_match"
#     params:
#       mcqa: true

kobalt_700:
  split: test
  subset: ["Syntax", "Semantics"] #, "Pragmatics", "Phonetics/Phonology", "Morphology"]
  params:
    num_samples: 100
    limit: 2
  evaluation:
    method: "string_match"
    params:
      mcqa: true

aime2025:
  split: test
  subset: ["AIME2025-I", "AIME2025-II"]
  params:
    num_samples: 15
    limit: 2
  evaluation:
    method: "math_match"
    params:
      extract_final_answer: true # 답변에서 최종 수식/결과를 추출합니다.

hrm8k:
  split: test
  subset: ["GSM8K", "KSM", "MATH", "MMMLU", "OMNI_MATH"]
  params:
    num_samples: 20
    limit: 2
  evaluation:
    method: "math_match"
    params:
      extract_final_answer: true # 답변에서 최종 수식/결과를 추출합니다.

hle:
  subset: ["Other", "Humanities/Social Science", "Math", "Physics", "Computer Science/AI", "Biology/Medicine", "Chemistry", "Engineering"]
  split: test
  params:
    num_samples: 15 # 총 8개 카테고리
    limit: 2
  evaluation:
    method: "string_match"

arc_agi:
  split: evaluation
  subset: default
  params:
    num_samples: 100
    limit: 2
  evaluation:
    method: "grid_match"

halluLens:
  split: test
  subset: [precise_wikiqa, mixed_entities, generated_entities]
  params:
    num_samples: 100
    limit: 2
  evaluation:
    abstention:
      model: gpt-4o-mini
    hallucination:
      model: gpt-4o

bfcl:
  split: test
  subset: [
    simple_python,
    simple_java,
    simple_javascript,
    multiple,
    irrelevance,
    live_simple,
    live_multiple,
    live_irrelevance,
    live_relevance,
    multi_turn_base,
    multi_turn_miss_func,
    multi_turn_miss_param,
  ]
  params:
    num_samples: 7
    limit: 3
  # 동시성 제어: BFCL 모델 inference 시 동시에 보낼 스레드 개수
  num_threads: 32
  evaluation:
    model: gpt-4o-2024-11-20

# ================= SWE-bench (integrated) =================
# SWE-bench Verified dataset for evaluating code patch generation
swebench:
  # Dataset configuration
  split: test
  subset: default
  # Dataset parameters (passed to Dataset constructor)
  params:
    num_samples: 77  # Maximum number of samples to evaluate
    limit: 3
    artifacts_path: horangi/horangi4-dataset/swebench_verified_official_80:v4
    dataset_dir: .

  # Model parameters override for this dataset
  # These will override the model config from <model_name>.yaml
  model_params:
    max_tokens: 32768  # Model limit (will be automatically capped per model)
    temperature: 0.0

  # Evaluation method
  evaluation:
    method: swebench
    params:
      # API server for running tests in Docker
      api_endpoint: https://api.nejumi-swebench.org/
      # api_endpoint: http://localhost:8000  # For local development
      timeout_sec: 1800  # 30 minutes per test
      concurrency: 2  # Number of parallel jobs
      namespace: swebench  # Docker image namespace
      tag: latest  # Docker image tag

  # Legacy settings (kept for backward compatibility with external/swe_bench)
  max_samples: 80
  max_workers: 2
  background_eval: false
  fc_enabled: true
  prebuild_images: false
  images:
    namespace: swebench
    tag: latest
  api_server:
    enabled: true
    endpoint: https://api.nejumi-swebench.org/
    timeout_sec: 1800
    concurrency: 2

# arc_agi2:
#   split: evaluation
#   subset: default
#   num_samples: 1000
#   evaluation:
#     method: "grid_match"
