# GPT-OSS-120B (vLLM Auto-Start)
#
# vLLM server will be automatically started/stopped by run_eval.py

wandb:
  run_name: "GPT-OSS-120B"

metadata:
  release_date: 2025-08-05
  size_category: "Large (30B<)"
  model_size: 120000000000          # GPT-OSS 120B
  active_params: 5000000000        # 5B (MoE)
  context_window: 131072

# =============================================================================
# vLLM Server Auto-Start Configuration
# =============================================================================

vllm:
  model_path: "openai/gpt-oss-120b"
  tensor_parallel_size: 4
  port: 8000
  host: "0.0.0.0"
  trust_remote_code: true
  served_model_name: "gpt-oss-120b"
  enable_auto_tool_choice: true
  tool_call_parser: "openai"
  reasoning_parser: "openai_gptoss"

# =============================================================================
# Model Configuration
# =============================================================================

model:
  name: gpt-oss-120b                 # Should match vllm.served_model_name
  client: litellm
  provider: hosted_vllm
  api_key_env: HOSTED_VLLM_API_KEY

  params:
    max_tokens: 16384
    temperature: 0.1
    top_p: 1.0

benchmarks:
  bfcl:
    use_native_tools: false           # Tool calling enabled
