# A.X-4.0 (vLLM, OpenAI-Compatible) - Thinking OFF

wandb:
  run_name: "A.X-4.0-Light"

metadata:
  release_date: "2025-04-30"
  size_category: "Small (<10B)"
  model_size: 7000000000
  active_params: 7000000000
  context_window: 16384

# vLLM Server Auto-Start Configuration
vllm:
  model_path: "skt/A.X-4.0-Light"
  tensor_parallel_size: 4
  port: 8002
  host: "0.0.0.0"
  max_model_len: 16384
  trust_remote_code: true
  served_model_name: "a.x-4.0-light"
  enable_auto_tool_choice: true
  tool_call_parser: "hermes"

model:
  name: a.x-4.0-light
  client: litellm
  provider: hosted_vllm
  api_key_env: HOSTED_VLLM_API_KEY

  params:
    max_tokens: 8192
    temperature: 0.6
    top_p: 0.95
    extra_body:
      chat_template_kwargs:
        enable_thinking: false

benchmarks:
  bfcl:
    use_native_tools: false  # vLLM tool_calls 형식 호환성 문제로 text-based 사용
