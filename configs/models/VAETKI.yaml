# VAETKI (External vLLM Server)
#
# NC-AI consortium의 한국어 Thinking 모델
# https://github.com/wbl-ncai/VAETKI
#
# 별도 환경에서 vLLM 서버를 수동으로 실행해야 합니다:
# pip install vllm==0.11.2
# pip install "git+https://github.com/wbl-ncai/VAETKI.git@releases/v1.0.0#subdirectory=vllm_plugin"
# python -m vllm.entrypoints.openai.api_server \
#   --model NC-AI-consortium-VAETKI/VAETKI \
#   --host 0.0.0.0 --port 8000 \
#   --tensor-parallel-size 4 --max-model-len 32768 \
#   --trust-remote-code --served-model-name VAETKI

wandb:
  run_name: "VAETKI"

metadata:
  release_date: "2025-01-22"
  size_category: "Large (30B<)"
  model_size: 112000000000        # 112B total
  active_params: 10100000000      # 10.1B active (MoE)
  context_window: 32768

model:
  name: VAETKI
  client: litellm
  provider: hosted_vllm
  base_url: http://localhost:8000/v1
  api_key_env: HOSTED_VLLM_API_KEY

  params:
    max_tokens: 16384
    temperature: 0.7
    top_p: 0.95
    top_k: 20

benchmarks:
  bfcl:
    use_native_tools: false
