# =============================================================================
# EXAONE-4.0.1-32B (RunPod vLLM)
# 
# Example configuration for serving open-source models via vLLM
# =============================================================================

# Model identifier (for display)
# vLLM uses the full HuggingFace path as model name
model_id: LGAI-EXAONE/EXAONE-4.0.1-32B

# API Provider: vLLM provides OpenAI-compatible API, so set to "openai"
# â†’ Passed to inspect eval as "openai/LGAI-EXAONE/EXAONE-4.0.1-32B"
api_provider: openai

# Model metadata (optional, for documentation and analysis)
metadata:
  provider: LG AI Research
  name: EXAONE-4.0.1-32B
  release_date: "2025-07-29"
  description: "Running on RunPod vLLM server"
  context_window: 32768
  max_output_tokens: 4096

# API settings
base_url: https://lgeyzkj4kubj2u-8000.proxy.runpod.net/v1
api_key_env: RUNPOD_API_KEY

# Default parameters (merged with base_config defaults)
defaults:
  temperature: 0.0
  max_tokens: 4096

# Benchmark-specific overrides
benchmarks:
  # BFCL: Text-based recommended for open-source models
  bfcl:
    use_native_tools: false
  
  # MT-Bench: Higher temperature for creative responses
  ko_mtbench:
    temperature: 0.7
