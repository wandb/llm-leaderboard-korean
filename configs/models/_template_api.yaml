# =============================================================================
# Model Configuration Template - API Version
# 
# Use this template for cloud API models (Anthropic, OpenAI, Google, xAI, etc.)
# Filename: <model-name>.yaml (e.g., gpt-4o.yaml, claude-opus-4-5.yaml)
# =============================================================================

# W&B Settings
wandb:
  run_name: "model-name"

# Model Metadata
metadata:
  release_date: "YYYY-MM-DD"
  size_category: "Large (30B<)"    # "Small (<10B)", "Medium (10B-30B)", "Large (30B<)"
  model_size: null                 # Total parameters (e.g., 70000000000 for 70B)
  active_params: null              # Active parameters for MoE models (same as model_size for dense)
  context_window: 128000
  max_output_tokens: 4096

# =============================================================================
# Model Configuration
# =============================================================================

model:
  # Model name (as recognized by the provider)
  # Examples:
  #   - claude-opus-4-5-20251101 (Anthropic)
  #   - gpt-4o-2024-11-20 (OpenAI)
  #   - gemini-2.5-pro (Google)
  name: model-name

  # API Client: litellm | openai
  client: litellm

  # Model Provider (for LiteLLM routing)
  # Examples: anthropic, openai, xai, google, qwen, friendliai
  provider: anthropic

  # Environment variable name for API key
  api_key_env: ANTHROPIC_API_KEY

  # Generation Parameters
  params:
    max_tokens: 4096
    temperature: 0.6
    top_p: 0.95
    reasoning_effort: high        # For reasoning models (high/low/xhigh)
    timeout: 3600                  # Request timeout (seconds)
    max_retries: 10                # Retry count on errors

# =============================================================================
# Benchmark-specific Overrides (optional)
# =============================================================================

benchmarks:
  bfcl:
    use_native_tools: false         # Native Tool Calling for API models
