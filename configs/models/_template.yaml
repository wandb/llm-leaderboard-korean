# =============================================================================
# Model Configuration Template
# 
# Copy this file when adding a new model.
# Filename: <model-name>.yaml (e.g., gpt-4o.yaml, llama-3.yaml)
# =============================================================================

# Model identifier
# Format: provider/model-name (e.g., openai/gpt-4o, upstage/solar-pro2)
model_id: provider/model-name

# API Provider (optional)
# For OpenAI-compatible APIs (vLLM, Upstage, etc.), set to "openai"
# This prepends "openai/" to the model name for inspect eval
# api_provider: openai

# Model metadata (optional, for documentation and analysis)
metadata:
  provider: Provider Name       # Provider (OpenAI, Anthropic, Upstage, etc.)
  name: Model Display Name      # Display name
  release_date: "YYYY-MM-DD"    # Release date
  size_category: None           # Size category ("Small (<10B)", "Medium (10-30B)", "Large (30B<)")
  model_size: None
  description: "Model description"
  context_window: 128000        # Context window size (tokens)
  max_output_tokens: 4096       # Maximum output tokens

# =============================================================================
# API Settings
# =============================================================================

# Custom API URL (for OpenAI-compatible APIs)
# Examples:
#   - vLLM server: http://YOUR_SERVER_IP:8000/v1
#   - Upstage: https://api.upstage.ai/v1
#   - RunPod: https://xxx.proxy.runpod.net/v1
# base_url: https://api.example.com/v1

# Environment variable name for API key
# The CLI will read the API key from this environment variable
api_key_env: YOUR_API_KEY_ENV

# =============================================================================
# Default Parameters
# =============================================================================
# These values override base_config.yaml defaults

defaults:
  temperature: 0.0    # Generation temperature (0.0 = deterministic)
  max_tokens: 4096    # Maximum generation tokens

# =============================================================================
# Benchmark-specific Overrides (optional)
# =============================================================================
# Use different settings for specific benchmarks

benchmarks:
  # BFCL: Function Calling benchmark
  # - use_native_tools: true  → Native Tool Calling (OpenAI, Claude, Gemini, etc.)
  # - use_native_tools: false → Text-based (recommended for open-source models)
  bfcl:
    use_native_tools: true
  
  # MT-Bench: May need higher temperature for creative responses
  # ko_mtbench:
  #   temperature: 0.7
  
  # SWE-bench: May need longer output
  # swebench_verified_official_80:
  #   max_tokens: 16384
