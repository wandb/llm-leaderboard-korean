[x] uv run run_eval.py --config vllm_test/qwen-qwen3-4b-openai-client
[o] uv run run_eval.py --config vllm_test/qwen-qwen3-4b-litellm-client


python -m spacy download ko_core_news_sm

openai/gpt-oss-20b
openai/gpt-oss-120b
LGAI-EXAONE/EXAONE-4.0-1.2B
LGAI-EXAONE/EXAONE-4.0-32B
LGAI-EXAONE/EXAONE-4.0.1-32B
skt/A.X-4.0
skt/A.X-4.0-Light
K-intelligence/Midm-2.0-Base-Instruct
K-intelligence/Midm-2.0-Mini-Instruct
Qwen/Qwen3-32B
google/gemma-3-1b-it
google/gemma-3-4b-it
google/gemma-3-12b-it
google/gemma-3-27b-it
google/gemma-3-270m-it
kanana-1.5-2.1b-instruct-2505
kanana-1.5-8b-instruct-2505
HyperCLOVAX-SEED-Text-Instruct-1.5B
HyperCLOVAX-SEED-Think-14B


uv run run_eval.py --config vllm_test/qwen-qwen3-4b-litellm-client

# Write here:
# Port assignments:
# 8000: openai-gpt-oss-20b
# 8001: openai-gpt-oss-120b
# 8002: lgai-exaone-exaone-4.0-1.2b
# 8003: lgai-exaone-exaone-4.0-32b
# 8004: lgai-exaone-exaone-4.0.1-32b
# 8005: skt-a.x-4.0
# 8006: skt-a.x-4.0-light
# 8007: k-intelligence-midm-2.0-base-instruct
# 8008: k-intelligence-midm-2.0-mini-instruct
# 8009: qwen-qwen3-32b
# 8010: google-gemma-3-1b-it
# 8011: google-gemma-3-4b-it
# 8012: google-gemma-3-12b-it
# 8013: google-gemma-3-27b-it
# 8014: google-gemma-3-270m-it
# 8015: kanana-1.5-2.1b-instruct-2505
# 8016: kanana-1.5-8b-instruct-2505
# 8017: hyperclovax-seed-text-instruct-1.5b
# 8018: hyperclovax-seed-think-14b

CUDA_VISIBLE_DEVICES=1 uv run run_eval.py --config vllm_litellm/google-gemma-3-270m-it
CUDA_VISIBLE_DEVICES=1 uv run run_eval.py --config vllm_litellm/google-gemma-3-1b-it
CUDA_VISIBLE_DEVICES=1 uv run run_eval.py --config vllm_litellm/google-gemma-3-4b-it
CUDA_VISIBLE_DEVICES=1 uv run run_eval.py --config vllm_litellm/google-gemma-3-12b-it
CUDA_VISIBLE_DEVICES=1 uv run run_eval.py --config vllm_litellm/google-gemma-3-27b-it
CUDA_VISIBLE_DEVICES=1 uv run run_eval.py --config vllm_litellm/qwen-qwen3-32b

CUDA_VISIBLE_DEVICES=1 uv run run_eval.py --config vllm_litellm/openai-gpt-oss-20b
CUDA_VISIBLE_DEVICES=1 uv run run_eval.py --config vllm_litellm/openai-gpt-oss-120b

CUDA_VISIBLE_DEVICES=2 uv run run_eval.py --config vllm_litellm/lgai-exaone-exaone-4.0-1.2b  # port: 8002
CUDA_VISIBLE_DEVICES=3 uv run run_eval.py --config vllm_litellm/lgai-exaone-exaone-4.0-32b  # port: 8003
CUDA_VISIBLE_DEVICES=4 uv run run_eval.py --config vllm_litellm/lgai-exaone-exaone-4.0.1-32b  # port: 8004

CUDA_VISIBLE_DEVICES=5,6 uv run run_eval.py --config vllm_litellm/skt-a.x-4.0  # port: 8005
CUDA_VISIBLE_DEVICES=2 uv run run_eval.py --config vllm_litellm/skt-a.x-4.0-light  # port: 8006

CUDA_VISIBLE_DEVICES=3 uv run run_eval.py --config vllm_litellm/k-intelligence-midm-2.0-base-instruct  # port: 8007
CUDA_VISIBLE_DEVICES= uv run run_eval.py --config vllm_litellm/k-intelligence-midm-2.0-mini-instruct  # port: 8008


CUDA_VISIBLE_DEVICES=6 uv run run_eval.py --config vllm_litellm/kanana-1.5-2.1b-instruct-2505  # port: 8015
uv run run_eval.py --config vllm_litellm/kanana-1.5-8b-instruct-2505  # port: 8016
[x] CUDA_VISIBLE_DEVICES=7 uv run run_eval.py --config vllm_litellm/hyperclovax-seed-text-instruct-1.5b  # port: 8017
[x] CUDA_VISIBLE_DEVICES=0 uv run run_eval.py --config vllm_litellm/hyperclovax-seed-think-14b  # port: 8018
