wandb:
  run_name: "LGAI-EXAONE/EXAONE-Deep-2.4B" # use run_name defined above

# if you don't use api, please set "api" as "false"
# if you use api, please select from "openai", "anthoropic", "google", "cohere", "vllm"
api: vllm
batch_size: 32 # vllmは256, apiは32を推奨 → CUDA 메모리 에러 방지를 위해 32로 변경

model:
  pretrained_model_name_or_path: "LGAI-EXAONE/EXAONE-Deep-2.4B" #if you use openai api, put the name of model
  chat_template: "LGAI-EXAONE/EXAONE-Deep-2.4B"
  size_category: "<10B"
  size: 2410000000
  release_date: "5/18/2025"
  max_model_len: 2500
