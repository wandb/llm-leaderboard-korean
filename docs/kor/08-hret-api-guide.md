# HRET API ê°€ì´ë“œ - MLOps ì¹œí™”ì  í‰ê°€ ì¸í„°í˜ì´ìŠ¤

HRET (Haerae Evaluation Toolkit)ëŠ” MLOps íŒŒì´í”„ë¼ì¸ê³¼ì˜ ì›í™œí•œ í†µí•©ì„ ìœ„í•´ ì„¤ê³„ëœ deepeval ìŠ¤íƒ€ì¼ì˜ ë°ì½”ë ˆì´í„° ê¸°ë°˜ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ê³ ìˆ˜ì¤€ ì¸í„°í˜ì´ìŠ¤ëŠ” ê¸°ì¡´ PipelineRunnerì™€ì˜ ì™„ì „í•œ í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ìµœì†Œí•œì˜ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì½”ë“œë¡œ LLM í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

```python
import llm_eval.hret as hret

# ê°„ë‹¨í•œ ë°ì½”ë ˆì´í„° ê¸°ë°˜ í‰ê°€
@hret.evaluate(dataset="kmmlu", model="huggingface")
def my_model(input_text: str) -> str:
    return model.generate(input_text)

# í‰ê°€ ì‹¤í–‰
result = my_model()
print(f"ì •í™•ë„: {result.metrics['accuracy']}")
```

## ğŸ“‹ í•µì‹¬ ê¸°ëŠ¥

### 1. ë°ì½”ë ˆì´í„° ê¸°ë°˜ API

#### `@hret.evaluate()`
ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ë‹¨ì¼ ëª¨ë¸ í•¨ìˆ˜ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

```python
@hret.evaluate(
    dataset="kmmlu", 
    model="huggingface", 
    evaluation_method="string_match"
)
def my_model(input_text: str) -> str:
    return model.generate(input_text)

result = my_model()  # EvaluationResult ë°˜í™˜
```

**ë§¤ê°œë³€ìˆ˜:**
- `dataset`: ë°ì´í„°ì…‹ ì´ë¦„ (ì˜ˆ: "kmmlu", "haerae", "benchhub")
- `model`: ëª¨ë¸ ë°±ì—”ë“œ ì´ë¦„ (ì˜ˆ: "huggingface", "openai")
- `evaluation_method`: í‰ê°€ ë°©ë²• (ì˜ˆ: "string_match", "llm_judge")
- `**kwargs`: ì¶”ê°€ ì„¤ì • ë§¤ê°œë³€ìˆ˜

#### `@hret.benchmark()`
ë™ì¼í•œ ë°ì´í„°ì…‹ì—ì„œ ì—¬ëŸ¬ ëª¨ë¸ì„ ë¹„êµí•©ë‹ˆë‹¤.

```python
@hret.benchmark(dataset="kmmlu")
def compare_models():
    return {
        "gpt-4": lambda x: gpt4_model.generate(x),
        "claude-3": lambda x: claude_model.generate(x),
        "custom": lambda x: custom_model.generate(x)
    }

results = compare_models()  # Dict[str, EvaluationResult] ë°˜í™˜
```

#### `@hret.track_metrics()`
í‰ê°€ í•¨ìˆ˜ì—ì„œ íŠ¹ì • ë©”íŠ¸ë¦­ì„ ì¶”ì í•©ë‹ˆë‹¤.

```python
@hret.track_metrics(["accuracy", "latency", "cost"])
def custom_evaluation():
    # í‰ê°€ ë¡œì§
    return {
        "accuracy": 0.85,
        "latency": 120.5,
        "cost": 0.02,
        "other_metric": "ì¶”ì ë˜ì§€_ì•ŠìŒ"
    }
```

### 2. ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¸í„°í˜ì´ìŠ¤

í‰ê°€ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ë” ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•œ ê²½ìš°:

```python
with hret.evaluation_context(
    dataset="kmmlu", 
    run_name="my_experiment"
) as ctx:
    # MLOps í†µí•© ì„¤ì •
    ctx.log_to_mlflow(experiment_name="llm_experiments")
    ctx.log_to_wandb(project_name="model_evaluation")
    
    # í‰ê°€ ì‹¤í–‰
    result = ctx.evaluate(my_model_function)
    
    # ê²°ê³¼ ì €ì¥
    ctx.save_results("experiment_results.json")
```

### 3. í¸ì˜ í•¨ìˆ˜

#### ë¹ ë¥¸ í‰ê°€
```python
result = hret.quick_eval(my_model_function, dataset="kmmlu")
```

#### ëª¨ë¸ ë¹„êµ
```python
models = {
    "baseline": lambda x: baseline_model.generate(x),
    "improved": lambda x: improved_model.generate(x)
}
results = hret.compare_models(models, dataset="kmmlu")
```

## ğŸ”§ ì„¤ì • ê´€ë¦¬

### ì „ì—­ ì„¤ì •

```python
hret.configure(
    default_dataset="kmmlu",
    default_model="huggingface",
    mlflow_tracking=True,
    wandb_tracking=True,
    output_dir="./results",
    auto_save_results=True,
    log_level="INFO"
)
```

### ì„¤ì • íŒŒì¼

`hret_config.yaml` íŒŒì¼ ìƒì„±:

```yaml
# HRET ì„¤ì •
default_dataset: "kmmlu"
default_model: "huggingface"
default_split: "test"
default_evaluation_method: "string_match"

# MLOps í†µí•©
mlflow_tracking: true
wandb_tracking: false
tensorboard_tracking: false

# ì¶œë ¥ ì„¤ì •
output_dir: "./hret_results"
auto_save_results: true
log_level: "INFO"

# ì„±ëŠ¥ ì„¤ì •
batch_size: 32
max_workers: 4
```

ì„¤ì • ë¡œë“œ:
```python
hret.load_config("hret_config.yaml")
```

## ğŸ”— MLOps í†µí•©

### MLflow í†µí•©

```python
with hret.evaluation_context(dataset="kmmlu") as ctx:
    ctx.log_to_mlflow(experiment_name="my_experiments")
    result = ctx.evaluate(my_model)
    # ê²°ê³¼ê°€ ìë™ìœ¼ë¡œ MLflowì— ë¡œê¹…ë©ë‹ˆë‹¤
```

### Weights & Biases í†µí•©

```python
with hret.evaluation_context(dataset="kmmlu") as ctx:
    ctx.log_to_wandb(project_name="llm_evaluation")
    result = ctx.evaluate(my_model)
    # ê²°ê³¼ê°€ ìë™ìœ¼ë¡œ W&Bì— ë¡œê¹…ë©ë‹ˆë‹¤
```

### ì»¤ìŠ¤í…€ í†µí•©

```python
def custom_logger(run_result, results):
    # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
    send_to_monitoring_system(run_result, results)

with hret.evaluation_context(dataset="kmmlu") as ctx:
    ctx.add_mlops_integration(custom_logger)
    result = ctx.evaluate(my_model)
```

## ğŸ“Š ë©”íŠ¸ë¦­ ì¶”ì  ë° ë¶„ì„

### ì‹¤í–‰ ê°„ ë©”íŠ¸ë¦­ ì¶”ì 

```python
# ëª¨ë“  í‰ê°€ ê¸°ë¡ ì¡°íšŒ
history = hret.get_metrics_history()
print(f"ì´ ì‹¤í–‰ íšŸìˆ˜: {len(history)}")

# ì‹¤í–‰ ê°„ íŠ¹ì • ë©”íŠ¸ë¦­ ë¹„êµ
accuracy_comparison = hret.compare_metric("accuracy")
print(f"ìµœê³  ì •í™•ë„: {accuracy_comparison['_stats']['best']}")
print(f"í‰ê·  ì •í™•ë„: {accuracy_comparison['_stats']['average']}")
```

### ê³ ê¸‰ ë©”íŠ¸ë¦­ ì¶”ì 

```python
tracker = hret.MetricsTracker()

# ì—¬ëŸ¬ ì‹¤í—˜ ì‹¤í–‰
for model_name, model_func in models.items():
    tracker.start_run(run_name=f"eval_{model_name}")
    
    result = hret.quick_eval(model_func, dataset="kmmlu")
    tracker.log_metrics(result.metrics)
    
    tracker.end_run()

# ê²°ê³¼ ë¹„êµ
comparison = tracker.compare_runs("accuracy")
```

## ğŸ­ MLOps íŒŒì´í”„ë¼ì¸ í†µí•©

### í›ˆë ¨ íŒŒì´í”„ë¼ì¸ í†µí•©

```python
class ModelTrainingPipeline:
    def train_and_evaluate(self, epochs=10):
        for epoch in range(1, epochs + 1):
            # ëª¨ë¸ í›ˆë ¨
            self.train_epoch(epoch)
            
            # ì²´í¬í¬ì¸íŠ¸ í‰ê°€
            if epoch % 3 == 0:
                self.evaluate_checkpoint(epoch)
    
    def evaluate_checkpoint(self, epoch):
        @hret.evaluate(dataset="kmmlu", model="huggingface")
        def checkpoint_model(input_text):
            return self.model.generate(input_text)
        
        with hret.evaluation_context(
            run_name=f"checkpoint_epoch_{epoch}"
        ) as ctx:
            ctx.log_to_mlflow(experiment_name="training")
            result = ctx.evaluate(checkpoint_model)
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            if self.detect_degradation(result):
                self.send_alert(epoch, result)
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
def hyperparameter_tuning():
    hyperparams = [
        {"lr": 0.001, "batch_size": 16, "dropout": 0.1},
        {"lr": 0.01, "batch_size": 32, "dropout": 0.2},
        {"lr": 0.005, "batch_size": 64, "dropout": 0.15},
    ]
    
    best_result = None
    best_score = 0
    
    for i, params in enumerate(hyperparams):
        model_func = create_model_with_params(params)
        
        with hret.evaluation_context(
            run_name=f"hyperparam_run_{i}"
        ) as ctx:
            # í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ê°€
            ctx.metrics_tracker.run_metadata.update({
                "hyperparameters": params
            })
            ctx.log_to_mlflow(experiment_name="hyperparameter_tuning")
            
            result = ctx.evaluate(model_func)
            
            if result.metrics["accuracy"] > best_score:
                best_score = result.metrics["accuracy"]
                best_result = (params, result)
    
    return best_result
```

### ì§€ì†ì  í‰ê°€

```python
class ContinuousEvaluation:
    def run_continuous_evaluation(self):
        def production_model(input_text):
            return call_production_api(input_text)
        
        with hret.evaluation_context(
            run_name=f"continuous_eval_{int(time.time())}"
        ) as ctx:
            ctx.log_to_mlflow(experiment_name="continuous_evaluation")
            
            result = ctx.evaluate(production_model)
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            if self.detect_degradation(result):
                self.send_performance_alert(result)
```

## ğŸ¯ ê³ ê¸‰ ì‚¬ìš© íŒ¨í„´

### ë‹¤ì¤‘ ë°ì´í„°ì…‹ í‰ê°€

```python
datasets = ["kmmlu", "haerae", "benchhub"]
results = {}

for dataset in datasets:
    with hret.evaluation_context(dataset_name=dataset) as ctx:
        ctx.log_to_mlflow(experiment_name="multi_dataset_eval")
        results[dataset] = ctx.evaluate(my_model)

# ë°ì´í„°ì…‹ë³„ ì„±ëŠ¥ ë¹„êµ
for dataset, result in results.items():
    print(f"{dataset}: {result.metrics['accuracy']:.3f}")
```

### A/B í…ŒìŠ¤íŠ¸

```python
@hret.benchmark(dataset="kmmlu")
def ab_test():
    return {
        "model_a": lambda x: model_a.generate(x),
        "model_b": lambda x: model_b.generate(x)
    }

results = ab_test()

# í†µê³„ì  ìœ ì˜ì„± ê²€ì •
from scipy import stats
scores_a = [s['accuracy'] for s in results['model_a'].samples]
scores_b = [s['accuracy'] for s in results['model_b'].samples]
t_stat, p_value = stats.ttest_ind(scores_a, scores_b)

print(f"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
print(f"ëª¨ë¸ A: {results['model_a'].metrics['accuracy']:.3f}")
print(f"ëª¨ë¸ B: {results['model_b'].metrics['accuracy']:.3f}")
print(f"P-ê°’: {p_value:.4f}")
```

### ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­

```python
def custom_evaluator(predictions, references):
    # ì»¤ìŠ¤í…€ í‰ê°€ ë¡œì§ êµ¬í˜„
    custom_scores = []
    for pred, ref in zip(predictions, references):
        score = calculate_custom_metric(pred, ref)
        custom_scores.append(score)
    
    return {
        "custom_metric": sum(custom_scores) / len(custom_scores),
        "custom_std": np.std(custom_scores)
    }

with hret.evaluation_context(
    dataset="kmmlu",
    evaluation_method_name="custom",
    evaluator_params={"custom_evaluator": custom_evaluator}
) as ctx:
    result = ctx.evaluate(my_model)
```

## ğŸ“š API ë ˆí¼ëŸ°ìŠ¤

### ì„¤ì • í´ë˜ìŠ¤

- **`HRETConfig`**: ì „ì—­ ì„¤ì • ê´€ë¦¬
  - `default_dataset`: ê¸°ë³¸ ë°ì´í„°ì…‹ ì´ë¦„
  - `default_model`: ê¸°ë³¸ ëª¨ë¸ ë°±ì—”ë“œ
  - `mlflow_tracking`: MLflow í†µí•© í™œì„±í™”
  - `wandb_tracking`: W&B í†µí•© í™œì„±í™”
  - `output_dir`: ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬
  - `auto_save_results`: ê²°ê³¼ ìë™ ì €ì¥

### í•µì‹¬ í´ë˜ìŠ¤

- **`EvaluationContext`**: í‰ê°€ ì„¸ì…˜ ê´€ë¦¬
  - `evaluate(model_function)`: ëª¨ë¸ í•¨ìˆ˜ í‰ê°€
  - `benchmark(model_functions)`: ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
  - `log_to_mlflow()`: MLflow í†µí•© ì¶”ê°€
  - `log_to_wandb()`: W&B í†µí•© ì¶”ê°€
  - `save_results()`: í‰ê°€ ê²°ê³¼ ì €ì¥

- **`MetricsTracker`**: ì‹¤í–‰ ê°„ ë©”íŠ¸ë¦­ ì¶”ì 
  - `start_run()`: ìƒˆ í‰ê°€ ì‹¤í–‰ ì‹œì‘
  - `log_metrics()`: í˜„ì¬ ì‹¤í–‰ì˜ ë©”íŠ¸ë¦­ ë¡œê¹…
  - `end_run()`: í˜„ì¬ ì‹¤í–‰ ì¢…ë£Œ ë° ê²°ê³¼ ì €ì¥
  - `compare_runs()`: ì‹¤í–‰ ê°„ ë©”íŠ¸ë¦­ ë¹„êµ

### ë°ì½”ë ˆì´í„°

- **`@evaluate()`**: ë‹¨ì¼ ëª¨ë¸ í‰ê°€ ë°ì½”ë ˆì´í„°
- **`@benchmark()`**: ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ ë°ì½”ë ˆì´í„°
- **`@track_metrics()`**: ë©”íŠ¸ë¦­ ì¶”ì  ë°ì½”ë ˆì´í„°

### ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

- **`configure()`**: ì „ì—­ ì„¤ì • ì§€ì •
- **`load_config()`**: íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
- **`quick_eval()`**: ë¹ ë¥¸ ëª¨ë¸ í‰ê°€
- **`compare_models()`**: ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
- **`evaluation_context()`**: í‰ê°€ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
- **`get_metrics_history()`**: í‰ê°€ ê¸°ë¡ ì¡°íšŒ
- **`compare_metric()`**: ì‹¤í–‰ ê°„ íŠ¹ì • ë©”íŠ¸ë¦­ ë¹„êµ

## ğŸ”„ PipelineRunnerì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜

HRETëŠ” ê¸°ì¡´ PipelineRunnerì™€ ì™„ì „í•œ í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤:

```python
# ê¸°ì¡´ ì½”ë“œ (ì—¬ì „íˆ ì‘ë™)
from llm_eval.runner import PipelineRunner
runner = PipelineRunner(
    dataset_name="kmmlu", 
    model_backend_name="huggingface"
)
result = runner.run()

# ìƒˆë¡œìš´ HRET ë°©ì‹
import llm_eval.hret as hret
result = hret.quick_eval(my_model_function, dataset="kmmlu")
```

## ğŸ› ï¸ ëª¨ë²” ì‚¬ë¡€

### 1. ì„¤ì • íŒŒì¼ ì‚¬ìš©
```python
# í”„ë¡œì íŠ¸ìš© hret_config.yaml ìƒì„±
hret.load_config("hret_config.yaml")
```

### 2. ì ì ˆí•œ ì˜¤ë¥˜ ì²˜ë¦¬ êµ¬í˜„
```python
@hret.evaluate(dataset="kmmlu")
def robust_model(input_text: str) -> str:
    try:
        return model.generate(input_text)
    except Exception as e:
        logger.error(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return ""  # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
```

### 3. ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ì—ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
```python
with hret.evaluation_context(dataset="kmmlu") as ctx:
    ctx.log_to_mlflow()
    # ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì—¬ëŸ¬ í‰ê°€
    result1 = ctx.evaluate(model1)
    result2 = ctx.evaluate(model2)
```

### 4. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ êµ¬í˜„
```python
def performance_monitor(run_result, results):
    accuracy = run_result["metrics"].get("accuracy", 0)
    if accuracy < PERFORMANCE_THRESHOLD:
        send_alert(f"ì„±ëŠ¥ ì €í•˜: {accuracy}")

with hret.evaluation_context() as ctx:
    ctx.add_mlops_integration(performance_monitor)
```

## ğŸ“– ì˜ˆì‹œ

ì™„ì „í•œ ì˜ˆì‹œëŠ” `examples/` ë””ë ‰í† ë¦¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- `examples/hret_examples.py`: ê¸°ë³¸ ì‚¬ìš©ë²• ì˜ˆì‹œ
- `examples/mlops_integration_example.py`: MLOps í†µí•© íŒ¨í„´
- `examples/hret_config.yaml`: ì„¤ì • íŒŒì¼ í…œí”Œë¦¿

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

HRETëŠ” í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. ìƒˆë¡œìš´ MLOps í†µí•© ì¶”ê°€
2. ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„
3. ìƒˆë¡œìš´ ë°ì½”ë ˆì´í„° íŒ¨í„´ ìƒì„±
4. ë¬¸ì„œ ë° ì˜ˆì‹œ ê°œì„ 

ìì„¸í•œ ê¸°ì—¬ ê°€ì´ë“œë¼ì¸ì€ [07-contribution-guide.md](07-contribution-guide.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

HRETëŠ” LLM í‰ê°€ë¥¼ ê°„ë‹¨í•˜ê³  ê°•ë ¥í•˜ë©° MLOps ì¤€ë¹„ê°€ ëœ ìƒíƒœë¡œ ë§Œë“­ë‹ˆë‹¤. ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” ë°ì½”ë ˆì´í„°ë¡œ ì‹œì‘í•˜ê³ , í”„ë¡œë•ì…˜ ë°°í¬ë¥¼ ìœ„í•´ì„œëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ì™€ ì„¤ì • íŒŒì¼ì„ í™œìš©í•˜ì„¸ìš”!