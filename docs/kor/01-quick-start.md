# ì†Œê°œ (Introduction)
HRET(HaeRae Evaluation Toolkit)ëŠ” í•œêµ­ì–´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ëŒ€í•´ í‘œì¤€í™”ëœ í‰ê°€í™˜ê²½ì—ì„œ í¬ê´„ì ì¸ ìœ íš¨ì„± ê²€ì¦ ê¸°ëŠ¥ì„ ì§€ì›í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. 

HRET í”„ë ˆì„ì›Œí¬ëŠ” ê¸°ì¡´ í•œêµ­ì–´ LLM í‰ê°€ ë°©ì‹ì´ ì¼ê´€ë˜ì§€ ì•Šì•„ì„œ ì§ì ‘ì ì¸ ë¹„êµê°€ ì–´ë ¤ì› ë˜ ê²ƒì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ëª©í‘œë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤.

## íŠ¹ì§• (Features)
- HRETëŠ” ì£¼ìš” í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬(HAE-RAE Bench, KMMLU, KUDGE, HRM8K ë“±)ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
- í‰ê°€ ê¸°ë²•(ë¬¸ìì—´ ì¼ì¹˜, ì–¸ì–´ ë¶ˆì¼ì¹˜ íŒ¨ë„í‹°, ë¡œê·¸ í™•ë¥  ê¸°ë°˜ í‰ê°€, LLM-as-judge)ì„ ì§€ì›í•©ë‹ˆë‹¤. 
  ë¡œì§“ ê¸°ë°˜ìœ¼ë¡œ í† í° ìˆ˜ì¤€ì˜ í™•ë¥ ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ ì‹ ë¢°ë„ í‰ê°€ê¹Œì§€ ê°€ëŠ¥í•˜ë©°, í•œê¸€ìœ¼ë¡œ ìš”ì²­í•œ ì‚¬í•­ì— ëŒ€í•´ ê·¸ì™¸ ì–¸ì–´ê°€ ë°œìƒí–ˆì„ ë•Œ ê²€ì¶œí•˜ì—¬ íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Test-time-scale(Beam Search, Best-of-N, Self-Consistency Voting)ì„ ì œê³µí•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì—¬ëŸ¬ ê°ë„ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- HuggingFaceë¥¼ í†µí•œ on-premise ì‚¬ìš© ë¿ë§Œ ì•„ë‹ˆë¼, litellm, openai-compatible apië¥¼ í†µí•´ 100+ê°œì˜ online inferenceì™€ ì—°ë™ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. 
- HRETëŠ” í•œêµ­ì–´ NLP ì—°êµ¬ì˜ ì¬í˜„ì„±ê³¼ íˆ¬ëª…ì„±ì„ í–¥ìƒì‹œí‚¤ê³ , ì¼ê´€ëœ ëŒ€ê·œëª¨ ì‹¤í—˜ í™˜ê²½ì„ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

---

# ì„¤ì¹˜ (Installation)
íŒŒì´ì¬(python >= 3.10) ê°€ìƒí™˜ê²½ êµ¬ì¶• í›„ ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. 
ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ í†µí•´ ì‹¤í–‰ í™˜ê²½ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê°€ìƒí™˜ê²½ êµ¬ì¶• (Conda ë˜ëŠ” Venv)
- git clone ëª…ë ¹ì–´ë¡œ HRET GitHub í”„ë¡œì íŠ¸ë¥¼ ë¡œì»¬ì— ë³µì‚¬í•´ì˜¤ê¸°
- ìš”êµ¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜

## Conda ê°€ìƒí™˜ê²½ (Virtual Environment) êµ¬í˜„ ì˜ˆì‹œ 
[1] ì•„ë‚˜ì½˜ë‹¤ ì„¤ì¹˜ https://www.anaconda.com/download   
   (ë‹¤ìš´ë¡œë“œ í˜ì´ì§€ ìš°ì¸¡ í•˜ë‹¨ì˜ skip registrationìœ¼ë¡œ ê°€ì…ì—†ì´ ì„¤ì¹˜ ê°€ëŠ¥)

[2] Anaconda prompt ì‹¤í–‰

[3] Conda í™˜ê²½ ìƒì„± ë° í™œì„±í™” (ì˜ˆì‹œ: python 3.11)
```bash
conda create -n hret python = 3.11 -y && conda activate hret
```

[4] git clone ìˆ˜í–‰ (ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ëŠ” working directoryë¡œ ì´ë™ í›„ ì‹¤í–‰)
```bash
git clone https://github.com/HAE-RAE/haerae-evaluation-toolkit.git
```

[5] git clone ì™„ë£Œëœ ë¡œì»¬ í´ë”ë¡œ ì´ë™
```bash
cd haerae-evaluation-toolkit
```


[6] requirements.txtë¡œ ìš”êµ¬ë˜ëŠ” íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```
---
# í™œìš© (Usage)

í™œìš©í•  ëª¨ë¸ ì„ ì • ë° í•„ìš”ì‹œ Access ê¶Œí•œ ìš”ì²­(Optional)
https://huggingface.co/models

---

## ì»¤ë§¨ë“œë¼ì¸ ì¸í„°í˜ì´ìŠ¤(CLI)ë¡œ í™œìš© (ex:google/gemma-3-1b-it)
```bash
python -m llm_eval.evaluator \
  --model huggingface \
  --model_params '{"model_name_or_path": "google/gemma-3-1b-it"}' \
  --dataset haerae_bench \
  --subset standard_nomenclature \
  --split test \
  --evaluation_method string_match \
  --output_file results.json
```
ìœ„ ì»¤ë§¨ë“œëŠ” ë‹¤ìŒ ì‚¬í•­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- haerae_bench (subset=csat_geo) í…ŒìŠ¤íŠ¸ ë¶„í• ì„ ë¡œë“œí•©ë‹ˆë‹¤.
- ìƒì„± ëª¨ë¸: huggingface â†’ google/gemma-3-1b-it
- string_matchë¥¼ í†µí•´ ìµœì¢… ì¶œë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
- ê²°ê³¼ JSON íŒŒì¼ì„ results.jsonì— ì €ì¥í•©ë‹ˆë‹¤.

---

## Evaluator API ì‚¬ìš©ë²•
- ë°±ì—”ë“œ ëª¨ë¸: ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ í†µí•´ ë¡œë“œë©ë‹ˆë‹¤ (huggingface, vllm ë“±).
- ë°ì´í„°ì…‹: ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ë¡œë“œë©ë‹ˆë‹¤ (ì˜ˆ: haerae_benchëŠ” ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤).
- LLM-as-a-Judge ë˜ëŠ” reward model ë¡œì§ì„ ì›í•˜ëŠ” ê²½ìš° judge_model ë° reward_modelì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ Noneì¸ ê²½ìš° ì‹œìŠ¤í…œì€ ë‹¨ì¼ ëª¨ë¸ ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- test-time-scalingì„ ìˆ˜í–‰í•˜ë ¤ë©´ ScalingMethodë¥¼ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- EvaluationMethod(ì˜ˆ: string_match, logit_based ë˜ëŠ” llm_judge)ëŠ” ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

ë‹¤ìŒì€ Evaluator ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³ , ëª¨ë¸ê³¼ (ì„ íƒì ìœ¼ë¡œ) ìŠ¤ì¼€ì¼ë§ ë°©ë²•ì„ ì ìš©í•œ ë‹¤ìŒ, í‰ê°€í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìµœì†Œí•œì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.

### Python Usage

```python
from llm_eval.evaluator import Evaluator

# 1) Initialize an Evaluator.
evaluator = Evaluator()

# 2) Run the evaluation pipeline
results = evaluator.run(
    model="huggingface",                        # or "litellm", "openai", etc.
    model_params={"model_name_or_path":"kakaocorp/kanana-nano-2.1b-instruct", "device":"cuda:0", "batch_size": 2, "max_new_tokens": 128}, # example HF Transformers param

    dataset="haerae_bench",                     # or "kmmlu", "qarv", ...
    subset=["standard_nomenclature"],            # optional subset(s)
    split="test",                               # "train"/"validation"/"test"
    dataset_params={},         # example HF config

    judge_model=None,                           # specify e.g. "huggingface_judge" if needed
    judge_params={},                            # params for judge model (if judge_model is not None)

    reward_model=None,                          # specify e.g. "huggingface_reward" if needed
    reward_params={},                           # params for reward model (if reward_model is not None)

    scaling_method=None,                        # or "beam_search", "best_of_n"
    scaling_params={},             # e.g., {"beam_size":3, "num_iterations":5}

    evaluator_params={}                         # e.g., custom evaluation settings
)

print(results)
# e.g. EvaluationResult(metrics={'accuracy': 0.0, 'language_penalizer_average': 0.8733333333333333}, info={'dataset_name': 'haerae_bench', 'subset': ['csat_geo'], 'split': 'test', 'model_backend_name': 'huggingface', 'scaling_method_name': None, 'evaluation_method_name': 'string_match', 'elapsed_time_sec': 1119.5369288921356}, samples=[...]

df = results.to_dataframe()
print(df) # input, reference, prediction, options, chain-of-thought, logits, ë“± í™•ì¸ ê°€ëŠ¥
```

## ğŸ‘ŒOutput í‰ê°€

### Raw ë°ì´í„° ì§ì ‘ ë¶„ì„í•˜ê¸°: to_dataframe()
ê°€ì¥ ê¸°ë³¸ì ì´ë©´ì„œ ê°•ë ¥í•œ ë¶„ì„ ë°©ë²•ì€ í‰ê°€ ê²°ê³¼ ì „ì²´ë¥¼ pandas.DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì§ì ‘ ë‹¤ë£¨ëŠ” ê²ƒì…ë‹ˆë‹¤. 
results.to_dataframe() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ í‰ê°€ ê³¼ì •ì˜ ëª¨ë“  ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¡œìš° í¬ë§·ìœ¼ë¡œ ë°›ì•„ì™€ ììœ ë¡­ê²Œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¯¸ë¦¬ ì •ì˜ëœ ë¦¬í¬íŠ¸ ì™¸ì— ìì‹ ë§Œì˜ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ê³  ì‹¶ì„ ë•Œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.

```python
from llm_eval.evaluator import Evaluator

evaluator = Evaluator()
results = evaluator.run(
    model="huggingface",
    model_params={"model_name_or_path": "google/gemma-2b-it"},
    dataset="haerae_bench",
    evaluation_method="string_match"
)
```

#### í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
```python
df = results.to_dataframe()
```

ë°ì´í„°í”„ë ˆì„ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìœ ìš©í•œ ì •ë³´ê°€ ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨ë˜ë©°, ì¶”ê°€ì ìœ¼ë¡œ ê° í‰ê°€ ì˜µì…˜ì— ë”°ë¥¸ ì¤‘ê°„ ê²°ê³¼ê°€ ì œê³µë©ë‹ˆë‹¤.

input: ëª¨ë¸ì— ì…ë ¥ëœ ê°’

prediction: ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€

reference: ì •ë‹µ ë ˆì´ë¸”

eval_is_correct: í‰ê°€ ê²°ê³¼ (True/False)

_subset_name: í•´ë‹¹ ìƒ˜í”Œì´ ì†í•œ ì„œë¸Œì…‹ ì´ë¦„

ì´ë¥¼ í™œìš©í•˜ì—¬ íŠ¹ì • ì¡°ê±´ì˜ ìƒ˜í”Œë§Œ í•„í„°ë§í•˜ê±°ë‚˜, ê·¸ë£¹ë³„ë¡œ í†µê³„ë¥¼ ë‚´ëŠ” ë“± pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë¬´í•œí•œ ê°€ëŠ¥ì„±ì˜ ì»¤ìŠ¤í…€ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ìë™ ë¶„ì„ ë¦¬í¬íŠ¸ í™œìš©í•˜ê¸°: analysis_report()
ë§¤ë²ˆ ì§ì ‘ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ê²ƒì´ ë²ˆê±°ë¡œìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ íˆ´í‚·ì€ í•œêµ­ì–´ì˜ íŠ¹ì„±ì— ë§ëŠ” ì£¼ìš” ë¶„ì„ í•­ëª©ë“¤ì„ ì¢…í•©í•˜ì—¬ ë³´ì—¬ì£¼ëŠ” ìë™í™”ëœ ë¦¬í¬íŒ… ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. analysis_report() ë©”ì„œë“œëŠ” í´ë¦­ í•œ ë²ˆìœ¼ë¡œ ì¢…í•©ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 

####  ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
```python
markdown_report = results.analysis_report()
print(markdown_report)
```

#### ì£¼ì˜: í•´ë‹¹ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒì˜ Spacy ëª¨ë¸ì´ í•„ìš”í•¨

```bash
python -m spacy download ko_core_news_sm
```

#### íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ í™•ì¸
```python
with open("analysis_report.md", "w", encoding="utf-8") as f:
    f.write(markdown_report)
```

ë¦¬í¬íŠ¸ í•´ì„í•˜ê¸° (Interpreting the Report)
ìë™ ìƒì„±ëœ ë¦¬í¬íŠ¸ì˜ ê° ì„¹ì…˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ë¶„ì„ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

ì¢…í•© ì„±ëŠ¥ ë¶„ì„ (Overall Performance Analysis): ì „ì²´ ì •í™•ë„, ì´ ìƒ˜í”Œ ìˆ˜ ë“± í•µì‹¬ ì„±ëŠ¥ ì§€í‘œë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

ì„œë¸Œì…‹ë³„ ì‹¬ì¸µ ë¶„ì„ (In-depth Analysis by Subset): ë°ì´í„°ì…‹ì˜ í•˜ìœ„ ê·¸ë£¹ë³„ ì •í™•ë„ë¥¼ ë¹„êµí•˜ì—¬ ëª¨ë¸ì˜ ê°•ì ê³¼ ì•½ì ì„ íŒŒì•…í•©ë‹ˆë‹¤.

ì–¸ì–´ì  í’ˆì§ˆ ë¶„ì„ (Linguistic Quality Analysis): ì •ë‹µ/ì˜¤ë‹µ ë‹µë³€ì˜ ì–´íœ˜ ë‹¤ì–‘ì„±(TTR)ì„ ë¹„êµí•˜ì—¬ ìƒì„±ë¬¼ì˜ ì–¸ì–´ì  í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.

ì˜¤ë‹µ ì›ì¸ ì¶”ë¡  ë¶„ì„ (Error Cause Inference Analysis): ì˜¤ë‹µì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œë‚˜ ì •ë‹µì—ì„œ ëˆ„ë½ëœ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë¸ì˜ ì‹¤íŒ¨ ì›ì¸ì„ ì¶”ì •í•©ë‹ˆë‹¤.

#### ê³ ê¸‰ ì‚¬ìš©ë²•
ë¦¬í¬íŠ¸ë¥¼ Markdown í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•˜ëŠ” ëŒ€ì‹ , ë¶„ì„ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°›ì•„ í›„ì† ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. output_format='dict' ì¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì´ëŠ” ìë™í™”ëœ ë¡œê¹…ì´ë‚˜ ì»¤ìŠ¤í…€ ì‹œê°í™”ë¥¼ êµ¬í˜„í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.


### ë°±ì—”ë“œ ëª¨ë¸ ë³€ê²½ - vllm (Backend model changed)
```python
from llm_eval.evaluator import Evaluator

# 1) Initialize an Evaluator.
evaluator = Evaluator()

# 2) Run the evaluation pipeline
results = evaluator.run(
model="openai",
model_params={"api_base": "http://0.0.0.0:8000/v1/chat/completions", "model_name": "Qwen/Qwen2.5-7B-Instruct", "batch_size" : 1},

dataset="haerae_bench",
split="test",
subset=["csat_geo"],

evaluation_method='string_match',
)

print(results)
# e.g. EvaluationResult(metrics={'accuracy': 0.34, 'language_penalizer_average': 0.4533333333333333}, info={'dataset_name': 'haerae_bench', 'subset': ['csat_geo'], 'split': 'test', 'model_backend_name': 'openai', 'scaling_method_name': None, 'evaluation_method_name': 'string_match', 'elapsed_time_sec': 49.80667734146118}, samples=[...])
```

---

## ğŸ” í‰ê°€ë°©ë²• (Evaluation)

### String / Partial Match Evaluation
ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’(prediction)ê³¼ ì°¸ì¡°ê°’(reference)ì´ 'ì™„ì „ ì¼ì¹˜(exact match)í•˜ëŠ”ì§€ / ë¶€ë¶„ ì¼ì¹˜(Partial Match)í•˜ëŠ”ì§€' í‰ê°€í•˜ëŠ” ë°©ì‹ì„ ë‘˜ ì¤‘ì—ì„œ ì„ íƒí•©ë‹ˆë‹¤.

#### Partial match
```python
from llm_eval.evaluator import Evaluator

# 1) Initialize an Evaluator.
evaluator = Evaluator()

# 2) Run the evaluation pipeline
results = evaluator.run(
model="huggingface",
model_params={"model_name_or_path":"Qwen/Qwen2.5-3B-Instruct", "device":"cuda:0", "batch_size": 2, "cot":True, "max_new_tokens": 1024},

dataset="haerae_bench",
split="test",
subset=["csat_geo"],

evaluation_method='partial_match',
)

print(results)
# e.g. EvaluationResult(metrics={'accuracy': 0.5866666666666667}, info={'dataset_name': 'haerae_bench', 'subset': ['csat_geo'], 'split': 'test', 'model_backend_name': 'huggingface', 'scaling_method_name': None, 'evaluation_method_name': 'partial_match', 'elapsed_time_sec': 2286.0827300548553}, samples=[...])
```

#### String match
```python
from llm_eval.evaluator import Evaluator

# 1) Initialize an Evaluator.
evaluator = Evaluator()

# 2) Run the evaluation pipeline
results = evaluator.run(
model="huggingface",
model_params={"model_name_or_path":"Qwen/Qwen2.5-3B-Instruct", "device":"cuda:0", "batch_size": 2, "cot":True, "max_new_tokens": 1024},

dataset="haerae_bench",
split="test",
subset=["csat_geo"],

evaluation_method='string_match',
)
```


### Log Probability Evaluation (ë¡œê·¸ í™•ë¥  í‰ê°€)
ëª¨ë¸ì´ ìƒì„±í•œ ê° ì„ íƒì§€ì˜ ë¡œê·¸ í™•ë¥ (log probability)ì„ ê¸°ë°˜ìœ¼ë¡œ ì •ë‹µì„ ì˜ˆì¸¡í•˜ê³ , ì´ë¥¼ ì°¸ì¡°ê°’(reference)ê³¼ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í‰ê°€ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¨ìˆœíˆ ëª¨ë¸ì˜ ì¶œë ¥(prediction)ë§Œì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , ëª¨ë¸ì˜ ë‚´ë¶€ í™•ë¥  ì •ë³´(ë¡œê·¸ í™•ë¥ )ë¥¼ í™œìš©í•˜ì—¬ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
answer_template = "{query} ### ë‹µ:"
results = evaluator.run(
model="huggingface",
model_params={"model_name_or_path":"kakaocorp/kanana-nano-2.1b-instruct", "device":"cuda:0", "batch_size": 4, "max_new_tokens": 128},

dataset="haerae_bench",
split="test",
evaluation_method='log_likelihood',
subset=["csat_geo"],
dataset_params = {"base_prompt_template" : answer_template},
)

print(results)
# e.g. EvaluationResult(metrics={'accuracy': 0.25333333333333335, 'language_penalizer_average': 0.0}, info={'dataset_name': 'haerae_bench', 'subset': ['csat_geo'], 'split': 'test', 'model_backend_name': 'huggingface', 'scaling_method_name': None, 'evaluation_method_name': 'log_likelihood', 'elapsed_time_sec': 84.34037137031555}, samples=[...])
```



---

## Scaling_method (ì„ íƒì ì„, ë°˜ë³µìœ¼ë¡œ ì¸í•´ ì¥ì‹œê°„ ì†Œìš”ë  ìˆ˜ ìˆìŒ)
- self_consistency: LLMì´ ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ ë²ˆ ë‹µë³€í•œ í›„, ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹µë³€ì„ ì„ íƒí•˜ëŠ” ê¸°ë²•


### Self_consistency
<í™œìš© ì½”ë“œ>
```python
from llm_eval.evaluator import Evaluator

# 1) Initialize an Evaluator.
evaluator = Evaluator()

# 2) Run the evaluation pipeline
results = evaluator.run(
model="huggingface",
model_params={"model_name_or_path":"Qwen/Qwen2.5-0.5B-Instruct", "device":"cuda", "batch_size": 1}, # example HF Transformers param

dataset="haerae_bench",
split="test",

scaling_method='self_consistency',
)
print(results)
# e.g. results['metrics'] : {'accuracy': 0.00040816326530612246}
```
---

## CoT (Chain of Thought)
CoTëŠ” ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ëª¨ë¸ì´ ë”°ë¥´ë„ë¡ ìœ ë„í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

### cot basic
<í™œìš© ì½”ë“œ>
```python
from llm_eval.evaluator import Evaluator

# 1) Initialize an Evaluator.
evaluator = Evaluator()

# 2) Run the evaluation pipeline
results = evaluator.run(
model="huggingface",
dataset="haerae_bench",
split="test",
subset=["csat_geo"],
model_params={"model_name_or_path":"Qwen/Qwen2.5-3B-Instruct", "device":"cuda:0", "batch_size": 2, "cot":True, "max_new_tokens": 512},
)

# e.g. EvaluationResult(metrics={'accuracy': 0.0}, info={'dataset_name': 'haerae_bench', 'subset': ['csat_geo'], 'split': 'test', 'model_backend_name': 'huggingface', 'scaling_method_name': None, 'evaluation_method_name': 'string_match', 'elapsed_time_sec': 1305.4367339611053}, samples=[...])

```

### cot_trigger (ì„ íƒì )
cot_triggerëŠ” "Chain-of-Thought (CoT)" ë°©ì‹ì˜ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì§€ì›í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ë¬¸ìì—´ íŠ¸ë¦¬ê±°ì…ë‹ˆë‹¤. cot=Trueë¡œ ì„¤ì •ëœ ê²½ìš°, cot_triggerê°€ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ê°€ëŠ¥í•©ë‹ˆë‹¤. cot_triggerëŠ” ëª¨ë¸ í”„ë¡¬í”„íŠ¸(prompt)ì— ì¶”ê°€ë˜ì–´ ëª¨ë¸ì´ ì²´ê³„ì ìœ¼ë¡œ ì‚¬ê³  ê³¼ì •ì„ í‘œí˜„í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, cot_triggerê°€ "Let's think step by step."ë¡œ ì„¤ì •ë˜ë©´, ëª¨ë¸ì€ ì…ë ¥ëœ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ê³  ë‹µì„ ìƒì„±í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤.

<í™œìš© ì½”ë“œ>
```python
from llm_eval.evaluator import Evaluator

# 1) Initialize an Evaluator.
evaluator = Evaluator()

# 2) Run the evaluation pipeline
results = evaluator.run(
model="huggingface",
dataset="haerae_bench",
split="test",
subset=["csat_geo"],
model_params={"model_name_or_path":"Qwen/Qwen2.5-3B-Instruct", "device":"cuda:0", "batch_size": 2, "cot":True, "cot_trigger": "Let's think step by step.", "max_new_tokens": 512},
)

print(results)
```

### cot_parser
cot_parserëŠ” pythonpathì•ˆì— í•¨ìˆ˜ê°€ ìœ„ì¹˜í•œ ê³³ ì´ë¦„ë§Œ ì ì–´ë‘ë©´, ìŠ¤ìŠ¤ë¡œ í•´ë‹¹ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì™€ì„œ parserë¡œ ì“¸ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥


---


### ì°¸ì¡°ë¬¸í—Œ (References)
- 'vLLM', https://github.com/vllm-project/vllm
- 'Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models', https://aclanthology.org/2024.acl-long.229.pdf

## FAQ
Q. ë‹¤ìŒ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ì¶œë ¥ë©ë‹ˆë‹¤: 'Make sure to have access to it at {model url} 403 Client Error. (Request ID: ~~ )'
A. í•´ë‹¹ ëª¨ë¸(ex: Llama, Gemma, etc)ì€ í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸ í›„ ëª¨ë¸ í˜ì´ì§€ ìƒë‹¨ì— ìˆëŠ” 
{Model Name} COMMUNITY LICENSE AGREEMENTì˜ í•˜ë‹¨ì— Expand to review and access í´ë¦­ í›„ ì •ë³´ ì…ë ¥í•œ ë‹¤ìŒ Submit í›„ í—ˆê°€ë¥¼ ë°›ì€ ë‹¤ìŒ (ì•½ 10ë¶„) ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### ğŸ“© Contact Us
- Development Lead: gksdnf424@gmail.com
- Research Lead: spthsrbwls123@yonsei.ac.kr

---

## ğŸ“œ License
Licensed under the Apache License 2.0.
Â© 2025 The HAE-RAE Team. All rights reserved.
