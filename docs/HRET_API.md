# HRET (Haerae Evaluation Toolkit) API

HRETëŠ” MLOps í™˜ê²½ì—ì„œ ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆëŠ” deepeval ìŠ¤íƒ€ì¼ì˜ ë°ì½”ë ˆì´í„° ê¸°ë°˜ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ APIëŠ” LLM í‰ê°€ë¥¼ ìœ„í•œ ê³ ìˆ˜ì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ë©°, ìµœì†Œí•œì˜ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì½”ë“œë¡œ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸš€ Quick Start

```python
import hret

# ê°„ë‹¨í•œ ë°ì½”ë ˆì´í„° ê¸°ë°˜ í‰ê°€
@hret.evaluate(dataset="kmmlu", model="huggingface")
def my_model(input_text: str) -> str:
    return model.generate(input_text)

# í‰ê°€ ì‹¤í–‰
result = my_model()
print(f"Accuracy: {result.metrics['accuracy']}")
```

## ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥

### 1. ë°ì½”ë ˆì´í„° ê¸°ë°˜ API

#### `@hret.evaluate()`
ë‹¨ì¼ ëª¨ë¸ í•¨ìˆ˜ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

```python
@hret.evaluate(dataset="kmmlu", model="huggingface", evaluation_method="string_match")
def my_model(input_text: str) -> str:
    return model.generate(input_text)

result = my_model()  # EvaluationResult ë°˜í™˜
```

#### `@hret.benchmark()`
ì—¬ëŸ¬ ëª¨ë¸ì„ ë¹„êµ í‰ê°€í•©ë‹ˆë‹¤.

```python
@hret.benchmark(dataset="kmmlu")
def compare_models():
    return {
        "gpt-4": lambda x: gpt4_model.generate(x),
        "claude-3": lambda x: claude_model.generate(x),
        "custom": lambda x: custom_model.generate(x)
    }

results = compare_models()  # Dict[str, EvaluationResult] ë°˜í™˜
```

#### `@hret.track_metrics()`
íŠ¹ì • ë©”íŠ¸ë¦­ì„ ì¶”ì í•©ë‹ˆë‹¤.

```python
@hret.track_metrics(["accuracy", "latency"])
def custom_evaluation():
    return {
        "accuracy": 0.85,
        "latency": 120.5,
        "other_metric": "not_tracked"
    }
```

### 2. ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €

ë” ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•œ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
with hret.evaluation_context(dataset="kmmlu", run_name="my_experiment") as ctx:
    # MLOps í†µí•© ì„¤ì •
    ctx.log_to_mlflow(experiment_name="llm_experiments")
    ctx.log_to_wandb(project_name="model_evaluation")
    
    # í‰ê°€ ì‹¤í–‰
    result = ctx.evaluate(my_model_function)
    
    # ê²°ê³¼ ì €ì¥
    ctx.save_results("experiment_results.json")
```

### 3. í¸ì˜ í•¨ìˆ˜

#### ë¹ ë¥¸ í‰ê°€
```python
result = hret.quick_eval(my_model_function, dataset="kmmlu")
```

#### ëª¨ë¸ ë¹„êµ
```python
models = {
    "model_a": lambda x: model_a.generate(x),
    "model_b": lambda x: model_b.generate(x)
}
results = hret.compare_models(models, dataset="kmmlu")
```

## ğŸ”§ ì„¤ì • ê´€ë¦¬

### ê¸€ë¡œë²Œ ì„¤ì •

```python
hret.configure(
    default_dataset="kmmlu",
    default_model="huggingface",
    mlflow_tracking=True,
    wandb_tracking=True,
    output_dir="./my_results",
    auto_save_results=True
)
```

### ì„¤ì • íŒŒì¼ ì‚¬ìš©

```python
# YAML ë˜ëŠ” JSON ì„¤ì • íŒŒì¼ ë¡œë“œ
hret.load_config("hret_config.yaml")
```

ì„¤ì • íŒŒì¼ ì˜ˆì‹œ (`hret_config.yaml`):
```yaml
default_dataset: "kmmlu"
default_model: "huggingface"
mlflow_tracking: true
wandb_tracking: true
output_dir: "./results"
auto_save_results: true
log_level: "INFO"
```

## ğŸ”— MLOps í†µí•©

### MLflow í†µí•©

```python
with hret.evaluation_context(dataset="kmmlu") as ctx:
    ctx.log_to_mlflow(experiment_name="my_experiments")
    result = ctx.evaluate(my_model)
    # ê²°ê³¼ê°€ ìë™ìœ¼ë¡œ MLflowì— ë¡œê¹…ë©ë‹ˆë‹¤
```

### Weights & Biases í†µí•©

```python
with hret.evaluation_context(dataset="kmmlu") as ctx:
    ctx.log_to_wandb(project_name="llm_evaluation")
    result = ctx.evaluate(my_model)
    # ê²°ê³¼ê°€ ìë™ìœ¼ë¡œ W&Bì— ë¡œê¹…ë©ë‹ˆë‹¤
```

### ì»¤ìŠ¤í…€ í†µí•©

```python
def custom_logger(run_result, results):
    # ì»¤ìŠ¤í…€ ë¡œê¹… ë¡œì§
    send_to_monitoring_system(run_result, results)

with hret.evaluation_context(dataset="kmmlu") as ctx:
    ctx.add_mlops_integration(custom_logger)
    result = ctx.evaluate(my_model)
```

## ğŸ“Š ë©”íŠ¸ë¦­ ì¶”ì 

### ì‹¤í–‰ ê¸°ë¡ ì¡°íšŒ

```python
# ëª¨ë“  ì‹¤í–‰ ê¸°ë¡ ì¡°íšŒ
history = hret.get_metrics_history()

# íŠ¹ì • ë©”íŠ¸ë¦­ ë¹„êµ
accuracy_comparison = hret.compare_metric("accuracy")
print(f"Best accuracy: {accuracy_comparison['_stats']['best']}")
```

### ë©”íŠ¸ë¦­ ì¶”ì ê¸° ì‚¬ìš©

```python
tracker = hret.MetricsTracker()

tracker.start_run("experiment_1")
tracker.log_metrics({"accuracy": 0.85, "f1": 0.82})
result = tracker.end_run()

# ì—¬ëŸ¬ ì‹¤í–‰ ë¹„êµ
comparison = tracker.compare_runs("accuracy")
```

## ğŸ­ MLOps íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ

### ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸

```python
class ModelTrainingPipeline:
    def train_and_evaluate(self, epochs=10):
        for epoch in range(1, epochs + 1):
            # ëª¨ë¸ í›ˆë ¨
            self.train_epoch(epoch)
            
            # ì²´í¬í¬ì¸íŠ¸ í‰ê°€
            if epoch % 3 == 0:
                self.evaluate_checkpoint(epoch)
    
    def evaluate_checkpoint(self, epoch):
        def model_function(input_text):
            return self.model.generate(input_text)
        
        with hret.evaluation_context(
            run_name=f"checkpoint_epoch_{epoch}"
        ) as ctx:
            ctx.log_to_mlflow(experiment_name="training")
            result = ctx.evaluate(model_function)
            
            # ì„±ëŠ¥ ì €í•˜ ê°ì§€
            if self.is_performance_degraded(result):
                self.send_alert(epoch, result)
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
def hyperparameter_tuning():
    hyperparams = [
        {"lr": 0.001, "batch_size": 16},
        {"lr": 0.01, "batch_size": 32},
        {"lr": 0.005, "batch_size": 64},
    ]
    
    best_result = None
    best_score = 0
    
    for i, params in enumerate(hyperparams):
        model_function = create_model_with_params(params)
        
        with hret.evaluation_context(
            run_name=f"hyperparam_run_{i}"
        ) as ctx:
            ctx.metrics_tracker.run_metadata.update({
                "hyperparameters": params
            })
            ctx.log_to_mlflow(experiment_name="hyperparameter_tuning")
            
            result = ctx.evaluate(model_function)
            
            if result.metrics["accuracy"] > best_score:
                best_score = result.metrics["accuracy"]
                best_result = (params, result)
    
    return best_result
```

### ì§€ì†ì  í‰ê°€

```python
class ContinuousEvaluation:
    def run_continuous_evaluation(self):
        def production_model(input_text):
            return call_production_api(input_text)
        
        with hret.evaluation_context(
            run_name=f"continuous_eval_{int(time.time())}"
        ) as ctx:
            ctx.log_to_mlflow(experiment_name="continuous_evaluation")
            
            result = ctx.evaluate(production_model)
            
            # ì„±ëŠ¥ ì €í•˜ ê°ì§€ ë° ì•Œë¦¼
            if self.detect_degradation(result):
                self.send_alert(result)
```

## ğŸ¯ ê³ ê¸‰ ì‚¬ìš©ë²•

### ë°°ì¹˜ ì²˜ë¦¬

```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
hret.configure(batch_size=32, max_workers=4)

with hret.evaluation_context(dataset="large_dataset") as ctx:
    result = ctx.evaluate(my_model)
```

### ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­

```python
def custom_evaluator(predictions, references):
    # ì»¤ìŠ¤í…€ í‰ê°€ ë¡œì§
    return {"custom_metric": calculate_custom_score(predictions, references)}

with hret.evaluation_context(
    dataset="kmmlu",
    evaluation_method_name="custom",
    evaluator_params={"custom_evaluator": custom_evaluator}
) as ctx:
    result = ctx.evaluate(my_model)
```

### ë‹¤ì¤‘ ë°ì´í„°ì…‹ í‰ê°€

```python
datasets = ["kmmlu", "haerae", "benchhub"]
results = {}

for dataset in datasets:
    with hret.evaluation_context(dataset_name=dataset) as ctx:
        ctx.log_to_mlflow(experiment_name="multi_dataset_eval")
        results[dataset] = ctx.evaluate(my_model)

# ë°ì´í„°ì…‹ë³„ ì„±ëŠ¥ ë¹„êµ
for dataset, result in results.items():
    print(f"{dataset}: {result.metrics['accuracy']:.3f}")
```

## ğŸ“š API ë ˆí¼ëŸ°ìŠ¤

### ì£¼ìš” í´ë˜ìŠ¤

- `HRETConfig`: ê¸€ë¡œë²Œ ì„¤ì • ê´€ë¦¬
- `EvaluationContext`: í‰ê°€ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
- `MetricsTracker`: ë©”íŠ¸ë¦­ ì¶”ì  ë° ë¹„êµ

### ì£¼ìš” í•¨ìˆ˜

- `configure(**kwargs)`: ê¸€ë¡œë²Œ ì„¤ì •
- `load_config(path)`: ì„¤ì • íŒŒì¼ ë¡œë“œ
- `evaluation_context()`: í‰ê°€ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
- `quick_eval()`: ë¹ ë¥¸ í‰ê°€
- `compare_models()`: ëª¨ë¸ ë¹„êµ
- `get_metrics_history()`: ì‹¤í–‰ ê¸°ë¡ ì¡°íšŒ
- `compare_metric()`: ë©”íŠ¸ë¦­ ë¹„êµ

### ë°ì½”ë ˆì´í„°

- `@evaluate()`: ë‹¨ì¼ ëª¨ë¸ í‰ê°€
- `@benchmark()`: ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ
- `@track_metrics()`: ë©”íŠ¸ë¦­ ì¶”ì 

## ğŸ” ì˜ˆì‹œ ì½”ë“œ

ì „ì²´ ì˜ˆì‹œ ì½”ë“œëŠ” ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¸ì¡°í•˜ì„¸ìš”:

- `examples/hret_examples.py`: ê¸°ë³¸ ì‚¬ìš©ë²• ì˜ˆì‹œ
- `examples/mlops_integration_example.py`: MLOps í†µí•© ì˜ˆì‹œ
- `examples/hret_config.yaml`: ì„¤ì • íŒŒì¼ ì˜ˆì‹œ

## ğŸ¤ ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±

HRETëŠ” ê¸°ì¡´ì˜ `PipelineRunner`ì™€ ì™„ì „íˆ í˜¸í™˜ë©ë‹ˆë‹¤. ê¸°ì¡´ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šê³ ë„ HRET APIë¥¼ ì ì§„ì ìœ¼ë¡œ ë„ì…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ê¸°ì¡´ ë°©ì‹ (ì—¬ì „íˆ ì‘ë™)
from llm_eval.runner import PipelineRunner
runner = PipelineRunner(dataset_name="kmmlu", model_backend_name="huggingface")
result = runner.run()

# ìƒˆë¡œìš´ HRET ë°©ì‹
import hret
result = hret.quick_eval(my_model_function, dataset="kmmlu")
```

## ğŸš€ ì‹œì‘í•˜ê¸°

1. HRET ëª¨ë“ˆ import
2. ëª¨ë¸ í•¨ìˆ˜ ì •ì˜
3. ë°ì½”ë ˆì´í„° ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
4. MLOps í†µí•© ì„¤ì • (ì„ íƒì‚¬í•­)
5. í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„

HRETë¥¼ ì‚¬ìš©í•˜ë©´ ë³µì¡í•œ LLM í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ APIë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!