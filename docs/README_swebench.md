# ğŸ”§ SWE-bench Evaluation Guide

SWE-bench is a benchmark that evaluates the ability to fix bugs in real open-source projects.  
It applies unified diff patches generated by models in a Docker environment and scores based on test pass/fail.

---

## ğŸ“‹ Table of Contents

- [Architecture](#architecture)
- [Server Installation and Running](#server-installation-and-running)
- [Client Configuration](#client-configuration)
- [Running Evaluations](#running-evaluations)
- [Dataset Information](#dataset-information)
- [Evaluation Flow](#evaluation-flow)
- [Output Format](#output-format-important)
- [Server API](#server-api-endpoints)

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ horangi (Client)            â”‚
â”‚  - Problem description â†’ LLMâ”‚
â”‚  - Generate unified diff    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP API
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SWE-bench Server            â”‚
â”‚  - Apply patch in Docker    â”‚
â”‚  - Run tests and score      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Server Installation and Running

The evaluation server must run on a **Linux environment with Docker installed**.

### Install Dependencies

```bash
# Method 1: Direct pip installation
pip install fastapi "uvicorn[standard]" swebench

# Method 2: Project optional dependency
uv add fastapi "uvicorn[standard]" swebench --optional swebench-server
```

### Start Server

```bash
# Start server
uv run python src/server/swebench_server.py --host 0.0.0.0 --port 8000

# Or run as module
uv run python -m server.swebench_server --host 0.0.0.0 --port 8000

# Run in background
nohup python src/server/swebench_server.py \
  --host 0.0.0.0 --port 8000 \
  >/tmp/swebench_server.out 2>&1 & disown
```

### Server Environment Variables

| Environment Variable | Default | Description |
|---------|--------|------|
| `SWE_API_KEY` | (none) | API authentication key (optional) |
| `SWE_MAX_JOBS` | `4` | Maximum concurrent jobs |
| `SWE_JOB_TIMEOUT` | `1800` | Job timeout (seconds, 30 minutes) |
| `SWE_PREBUILD_IMAGES` | `true` | Pre-build Docker images |

---

## Client Configuration

The client (horangi) can run on macOS or other environments.

### Environment Variables

```bash
# Set server URL
export SWE_SERVER_URL=http://YOUR_SERVER:8000

# (Optional) If API key is set
export SWE_API_KEY=your-api-key
```

### Configuration File (`configs/base_config.yaml`)

```yaml
benchmarks:
  swebench:
    server_url: http://YOUR_SERVER:8000
    timeout: 1800  # 30 minutes
```

---

## Running Evaluations

```bash
# Single run (test)
uv run horangi swebench_verified_official_80 --config gpt-4o -T limit=5

# Full evaluation (80 samples)
uv run horangi swebench_verified_official_80 --config gpt-4o
```

---

## Dataset Information

| Name | Description | Samples | Input Tokens |
|-----|------|--------:|----------|
| `swebench_verified_official_80` | 80 verified instances | 80 | < 7,000 |

> **Note**: This is a subset sampled from the original SWE-bench Verified (500), filtered for input tokens under 7,000 while maintaining difficulty distribution.

### Difficulty Distribution

| Difficulty | Original (500) | 80 Subset |
|-------|----------:|------------:|
| < 15 min | 38.8% | 46.2% |
| 15 min ~ 1 hour | 52.2% | 50.0% |
| 1~4 hours | 8.4% | 3.8% |
| > 4 hours | 0.6% | 0.0% |

---

## Evaluation Flow

1. **Input**: Problem description (Issue), hints, relevant code
2. **Generation**: Model generates unified diff patch
3. **Application**: Server applies patch in Docker environment
4. **Scoring**: Run tests and determine Pass/Fail

### Detailed Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset (80 items)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Input (Issue/PR context, relevant snippets,
                â”‚         reproduction/expected tests, constraints)
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generation (LLM)                       â”‚
â”‚  - Prompt shaping (CRITICAL sentence)  â”‚
â”‚  - Generate unified diff               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Unified diff (line numbers in @@ required)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Preprocessing (expansion & normalization) â”‚
â”‚  - Extract minimal patch                â”‚
â”‚  - Hunk header expansion                â”‚
â”‚  - Filename normalization / merge dups  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Apply patch (git apply / patch --fuzz)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Evaluation runner                      â”‚
â”‚  - Run in Docker environment           â”‚
â”‚  - Execute unit tests                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Pass/Fail
                    â–¼
            Resolved / Not Resolved
                    â†“
             SWE-Bench Score
           (Resolved rate = pass rate)
```

---

## Output Format (Important!)

The model must generate a unified diff with **correct hunk headers**:

```diff
--- a/file.py
+++ b/file.py
@@ -10,5 +10,7 @@
 def function():
-    old_code()
+    new_code()
+    additional_fix()
```

### âš ï¸ CRITICAL

- **Line numbers required**: Hunk headers must be in `@@ -start,count +start,count @@` format.
- Using only `@@ @@` without line numbers will cause patch application to fail.

### Correct Example

```diff
--- a/astropy/modeling/separable.py
+++ b/astropy/modeling/separable.py
@@ -245,1 +245,1 @@
-        cright[-right.shape[0]:, -right.shape[1]:] = 1
+        cright[-right.shape[0]:, -right.shape[1]:] = right
```

---

## Server API Endpoints

| Endpoint | Method | Description |
|-----------|--------|------|
| `/health` | GET | Health check |
| `/v1/jobs` | POST | Create evaluation job |
| `/v1/jobs/{job_id}` | GET | Query job status |
| `/v1/jobs/{job_id}/report` | GET | Query evaluation results |

### Job Creation Example

```bash
curl -X POST http://localhost:8000/v1/jobs \
  -H "Content-Type: application/json" \
  -d '{
    "instance_id": "astropy__astropy-12907",
    "patch_diff": "--- a/file.py\n+++ b/file.py\n@@ -1,1 +1,1 @@\n-old\n+new",
    "model_name_or_path": "gpt-4o"
  }'
```

### Response Example

```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending"
}
```

### Query Job Status

```bash
curl http://localhost:8000/v1/jobs/{job_id}
```

```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "finished",
  "instance_id": "astropy__astropy-12907",
  "created_at": 1702800000.0,
  "finished_at": 1702800300.0
}
```

### Query Results

```bash
curl http://localhost:8000/v1/jobs/{job_id}/report
```

```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "instance_id": "astropy__astropy-12907",
  "resolved_ids": ["astropy__astropy-12907"],
  "unresolved_ids": [],
  "error_ids": []
}
```

---

## Troubleshooting

### Docker Related

- The server requires Docker. Run on a Linux environment.
- On macOS, run only the client and operate the server on a separate Linux server.

### Patch Application Failure

- Check if hunk headers are missing line numbers.
- If `git apply` fails, the server automatically tries `patch --fuzz=10/20`.

### Timeout

- Default timeout is 30 minutes (1800 seconds).
- Complex tests may take longer; adjust with the `SWE_JOB_TIMEOUT` environment variable.

---

## References

- [SWE-bench Official Repository](https://github.com/princeton-nlp/SWE-bench)
- [Nejumi LLM Leaderboard SWE-bench Guide](https://github.com/wandb/llm-leaderboard/blob/main/docs/README_swebench.md)

