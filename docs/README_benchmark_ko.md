# ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€ ê°€ì´ë“œ

> ìƒˆ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
> ì„¤ì¹˜, ì‚¬ìš©ë²•, ëª¨ë¸ ì„¤ì •ì€ [ë£¨íŠ¸ README](../README.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

---

## ğŸ¯ ìƒˆ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€

### Step 1: Config íŒŒì¼ ìƒì„±

```python
# src/benchmarks/my_benchmark.py
from core.benchmark_config import BenchmarkConfig

CONFIG = BenchmarkConfig(
    # ë°ì´í„° ì†ŒìŠ¤ (í•„ìˆ˜)
    data_type="weave",  # "weave" ë˜ëŠ” "jsonl"
    data_source="weave:///entity/project/object/DatasetName:latest",
    
    # í•„ë“œ ë§¤í•‘
    field_mapping={
        "id": "id",
        "input": "question",
        "target": "answer",
        "choices": "options",  # MCQAë§Œ
    },
    
    # í‰ê°€ ì„¤ì •
    answer_format="index_0",
    solver="multiple_choice",
    scorer="choice",
    system_message="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
)
```

### Step 2: ë“±ë¡

```python
# src/benchmarks/__init__.py
from benchmarks.my_benchmark import CONFIG as my_benchmark

BENCHMARKS = {
    ...
    "my_benchmark": my_benchmark,
}

BENCHMARK_DESCRIPTIONS = {
    ...
    "my_benchmark": "ë²¤ì¹˜ë§ˆí¬ ì„¤ëª…",
}
```

### Step 3: Task í•¨ìˆ˜ ì¶”ê°€

```python
# horangi.py (ë£¨íŠ¸)
@task
def my_benchmark(shuffle: bool = False, limit: int | None = None) -> Task:
    """My Benchmark"""
    return create_benchmark(name="my_benchmark", shuffle=shuffle, limit=limit)
```

### Step 4: í…ŒìŠ¤íŠ¸

```bash
uv run horangi my_benchmark --model openai/gpt-4o -T limit=5
```

---

## ğŸ“‹ BenchmarkConfig í•„ë“œ ì°¸ì¡°

### í•„ìˆ˜ í•„ë“œ

| í•„ë“œ | ì„¤ëª… |
|------|------|
| `data_type` | `"weave"` ë˜ëŠ” `"jsonl"` |
| `data_source` | Weave URI ë˜ëŠ” JSONL íŒŒì¼ëª… (`src/data/` ê¸°ì¤€) |

### ì£¼ìš” ì„ íƒ í•„ë“œ

| í•„ë“œ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `field_mapping` | `{}` | ë°ì´í„°ì…‹ â†’ Sample í•„ë“œ ë§¤í•‘ |
| `solver` | `"multiple_choice"` | Solver |
| `scorer` | `"choice"` | Scorer |
| `answer_format` | `"index_0"` | ì •ë‹µ ë³€í™˜ ë°©ì‹ |
| `system_message` | `None` | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ |

### `answer_format` ì˜µì…˜

| ê°’ | ì„¤ëª… | ì˜ˆì‹œ |
|----|------|------|
| `identity` | ë³€í™˜ ì—†ìŒ | `"ì •ë‹µ"` â†’ `"ì •ë‹µ"` |
| `index_0` | 0-indexed â†’ A/B/C | `0` â†’ `"A"` |
| `index_1` | 1-indexed â†’ A/B/C | `1` â†’ `"A"` |
| `text` | í…ìŠ¤íŠ¸ â†’ ì¸ë±ìŠ¤ | `"ì‚¬ê³¼"` â†’ `"A"` |
| `letter` | ê·¸ëŒ€ë¡œ ìœ ì§€ | `"A"` â†’ `"A"` |
| `to_string` | ë¬¸ìì—´ë¡œ ë³€í™˜ | `123` â†’ `"123"` |
| `boolean` | True/False â†’ A/B | `True` â†’ `"A"` |

### Solver / Scorer ì˜µì…˜

| Solver | ìš©ë„ |
|--------|------|
| `multiple_choice` | MCQA |
| `generate` | ììœ  ìƒì„± |
| `bfcl_solver` | Tool calling (Native) |
| `bfcl_text_solver` | Tool calling (Text-based) |
| `mtbench_solver` | MT-Bench ë©€í‹°í„´ ëŒ€í™” |
| `swebench_patch_solver` | SWE-bench |

| Scorer | ìš©ë„ |
|--------|------|
| `choice` | MCQA ì •í™•ë„ |
| `match` | ì •í™• ì¼ì¹˜ |
| `match_numeric` | ìˆ«ì ì¼ì¹˜ |
| `model_graded_qa` | LLM ì±„ì  |
| `hle_grader` | HLE ì „ìš© ì±„ì  |
| `grid_match` | ê·¸ë¦¬ë“œ ì¼ì¹˜ (ARC-AGI) |
| `macro_f1` | Macro F1 |
| `kobbq_scorer` | KoBBQ í¸í–¥ì„± |
| `hallulens_qa_scorer` | HalluLens QA |
| `refusal_scorer` | HalluLens ê±°ë¶€ ì‘ë‹µ í‰ê°€ |
| `bfcl_scorer` | BFCL í•¨ìˆ˜í˜¸ì¶œ |
| `mtbench_scorer` | MT-Bench í‰ê°€ |
| `swebench_server_scorer` | SWE-bench ì„œë²„ ì±„ì  |

---

## ğŸ”§ ì»¤ìŠ¤í…€ Scorer ì¶”ê°€

### Step 1: Scorer íŒŒì¼ ìƒì„±

```python
# src/scorers/my_scorer.py
from inspect_ai.scorer import Score, Scorer, Target, scorer, accuracy, CORRECT, INCORRECT
from inspect_ai.solver import TaskState

@scorer(metrics=[accuracy()])
def my_scorer() -> Scorer:
    async def score(state: TaskState, target: Target) -> Score:
        response = state.output.completion
        expected = target.text
        is_correct = response.strip() == expected.strip()
        
        return Score(
            value=CORRECT if is_correct else INCORRECT,
            answer=response[:100],
        )
    return score
```

### Step 2: ë“±ë¡

```python
# src/scorers/__init__.py
from scorers.my_scorer import my_scorer

__all__ = [..., "my_scorer"]
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

ìƒˆ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€ ì‹œ:

- [ ] `src/benchmarks/`ì— config íŒŒì¼ ìƒì„±
- [ ] `src/benchmarks/__init__.py`ì— ë“±ë¡
- [ ] `horangi.py`ì— `@task` í•¨ìˆ˜ ì¶”ê°€
- [ ] í…ŒìŠ¤íŠ¸ ì‹¤í–‰

---

## ğŸ”— ì°¸ê³ 

- [Inspect AI Docs](https://inspect.ai-safety-institute.org.uk/)
- [inspect_evals GitHub](https://github.com/UKGovernmentBEIS/inspect_evals)

